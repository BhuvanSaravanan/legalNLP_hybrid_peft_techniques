{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login, create_repo, upload_folder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to Hugging Face Hub\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN -Lora\n","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"eval_lora_model.py\n\nEvaluates a pre-trained LoRA model on the test set and saves metrics to JSON.\n\"\"\"\n\nfrom huggingface_hub import hf_hub_download\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nfrom peft import LoraConfig, get_peft_model\nimport random\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nclass Config:\n    # Update these for LoRA model\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    context_hidden_size = 256\n    max_num_sentences = 48\n    max_length = 128\n    dropout_rate = 0.4\n    gamma = 2.0\n    batch_size = 64\n\n    # LoRA specific parameters\n    lora_r = 128\n    lora_alpha = 64\n    lora_dropout = 0.15\n    lora_target_modules = [\"query\", \"value\", \"key\"]\n\n    # Paths and repo info\n    output_dir = \"./lora_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-lora-final\"  # Your LoRA model repo\n\n\n# Load datasets\nsplits = {\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    dataset = dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n    return dataset\n\ndef tokenize_single_dataset(dataset, tokenizer, label2id):\n    \"\"\"Tokenize dataset for hierarchical input\"\"\"\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n\n    tokenized_ds = dataset.map(tokenize_document)\n    # Set format to PyTorch tensors\n    tokenized_ds.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    return tokenized_ds\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.dropout = nn.Dropout(Config.dropout_rate)\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return self.dropout(x + self.position_emb(positions))\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling\"\"\"\n    def __init__(self, d_model, nhead=8, dim_feedforward=1024, dropout=0.2):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n            activation='gelu'\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            self.encoder_layer,\n            num_layers=2\n        )\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with residual connection\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.3):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, input_size*2)\n        self.linear2 = nn.Linear(input_size*2, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n        self.layer_norm = nn.LayerNorm(input_size*2)\n        self.residual_proj = nn.Linear(input_size, num_labels)\n\n    def forward(self, x):\n        residual = x\n        x = self.linear1(x)\n        x = self.layer_norm(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        return self.linear2(x) + self.residual_proj(residual)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        # Compute standard CRF loss\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n\n        # Apply focal loss transformation\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        # Apply class weights if provided\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]  # (batch_size, seq_len)\n            valid_counts = mask.sum(dim=1)  # (batch_size,)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass LoRAHSLNModel(nn.Module):\n    \"\"\"Hierarchical model with LoRA integration\"\"\"\n    def __init__(self, num_labels, model_config):\n        super().__init__()\n        # Update config from model_config\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        # Sentence encoding with LoRA\n        base_bert = AutoModel.from_pretrained(Config.bert_model_name)\n\n        # Configure LoRA\n        lora_config = LoraConfig(\n            r=Config.lora_r,\n            lora_alpha=Config.lora_alpha,\n            target_modules=Config.lora_target_modules,\n            lora_dropout=Config.lora_dropout,\n            bias=\"none\"\n        )\n        self.bert = get_peft_model(base_bert, lora_config)\n\n        # Enhanced feature extraction\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.sent_projection = nn.Linear(\n            self.bert.config.hidden_size,\n            self.bert.config.hidden_size\n        )\n\n        # Context encoding\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n\n        # Emission layer\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n\n        # CRF layer with focal loss\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_projection(sent_emb)\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\ndef plot_confusion_matrix(y_true, y_pred, labels, label_names, output_dir):\n    \"\"\"Generate and save a confusion matrix with raw counts and improved visibility\"\"\"\n    # Compute confusion matrix with raw counts\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    \n    # Create a figure with optimized dimensions\n    plt.figure(figsize=(10, 8))\n    ax = plt.subplot()\n    \n    # Set font properties for better visibility\n    label_font = {'size': 10, 'weight': 'bold'}\n    value_font = {'size': 9, 'weight': 'bold'}\n    \n    # Plot heatmap with bold integer annotations\n    sns.heatmap(\n        cm, \n        annot=True, \n        fmt='d',  # Integer format\n        cmap=\"Blues\", \n        xticklabels=label_names,\n        yticklabels=label_names,\n        cbar=True,\n        ax=ax,\n        annot_kws=value_font  # Apply bold to cell values\n    )\n    \n    # Set labels and title with bold font\n    plt.title('Confusion Matrix (Raw Counts)', fontsize=12, fontweight='bold')\n    plt.ylabel('True Label', fontdict=label_font)\n    plt.xlabel('Predicted Label', fontdict=label_font)\n    \n    # Make axis labels bold and rotated for readability\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=9, fontweight='bold')\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=9, fontweight='bold')\n    \n    # Adjust colorbar font\n    cbar = ax.collections[0].colorbar\n    cbar.ax.tick_params(labelsize=9)\n    \n    # Save the plot with tight layout\n    plot_path = os.path.join(output_dir, \"confusion_matrix.png\")\n    plt.tight_layout()\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    plt.close()\n    \n    return plot_path\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    model.eval()\n    all_preds, all_labels = [], []\n    total_time = 0\n    n_docs = 0\n    n_sentences = 0\n    eval_start = time.time()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            mask = attention_mask[:, :, 0] > 0\n\n            start = time.time()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            end = time.time()\n\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=mask)\n\n            for i in range(len(labels)):\n                seq_len = mask[i].sum().item()\n                all_preds.extend(preds[i][:seq_len])\n                all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n            total_time += (end - start)\n            n_docs += input_ids.shape[0]\n            n_sentences += mask.sum().item()\n\n    eval_end = time.time()\n    eval_time = eval_end - eval_start\n\n    labels_for_report = list(range(len(label_list)))\n    target_names = label_list\n\n    # Generate classification report\n    report = classification_report(\n        all_labels, all_preds,\n        labels=labels_for_report,\n        target_names=target_names,\n        output_dict=True,\n        zero_division=0\n    )\n\n    # Calculate additional metrics\n    macro_precision = precision_score(\n        all_labels, all_preds, average='macro', zero_division=0\n    )\n    macro_recall = recall_score(\n        all_labels, all_preds, average='macro', zero_division=0\n    )\n    weighted_precision = precision_score(\n        all_labels, all_preds, average='weighted', zero_division=0\n    )\n    weighted_recall = recall_score(\n        all_labels, all_preds, average='weighted', zero_division=0\n    )\n    per_label_precision = {\n        label: report[label]['precision']\n        for label in label_list\n    }\n    per_label_recall = {\n        label: report[label]['recall']\n        for label in label_list\n    }\n\n    macro_f1 = report['macro avg']['f1-score']\n    weighted_f1 = report['weighted avg']['f1-score']\n    accuracy = accuracy_score(all_labels, all_preds)\n    per_label_f1 = {\n        label: report[label]['f1-score']\n        for label in label_list\n    }\n\n    latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n    latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n    return {\n        \"macro_f1\": macro_f1,\n        \"weighted_f1\": weighted_f1,\n        \"accuracy\": accuracy,\n        \"per_label_f1\": per_label_f1,\n        \"macro_precision\": macro_precision,\n        \"macro_recall\": macro_recall,\n        \"weighted_precision\": weighted_precision,\n        \"weighted_recall\": weighted_recall,\n        \"per_label_precision\": per_label_precision,\n        \"per_label_recall\": per_label_recall,\n        \"latency_ms_per_doc\": latency_doc,\n        \"latency_ms_per_sentence\": latency_sent,\n        \"eval_time_seconds\": eval_time,\n        \"num_samples\": n_docs,\n        \"all_labels\": all_labels,\n        \"all_preds\": all_preds\n    }\n\ndef evaluate_lora_model():\n    \"\"\"Loads a pre-trained LoRA model and evaluates it on the test set\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING LoRA MODEL EVALUATION\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"Seed: {SEED}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts from Hugging Face Hub\n        print(\"Downloading LoRA model artifacts from Hugging Face Hub...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n\n        # Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n\n        label2id = saved_config['label2id']\n        id2label = saved_config['id2label']\n        model_config = saved_config['model_config']\n\n        # Convert id2label keys to integers\n        id2label = {int(k): v for k, v in id2label.items()}\n\n        # Create label list sorted by ID\n        label_list = [id2label[i] for i in range(len(id2label))]\n\n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        print(f\"Loaded configuration for LoRA model: {Config.bert_model_name}\")\n        print(f\"LoRA r: {Config.lora_r}, alpha: {Config.lora_alpha}\")\n        print(f\"Number of labels: {len(label_list)}\")\n\n        # 2. Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n        # 3. Preprocess test dataset\n        print(\"\\nPreprocessing test dataset...\")\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n\n        # 4. Tokenize test dataset\n        print(\"Tokenizing test dataset...\")\n        test_tokenized = tokenize_single_dataset(test_hier, tokenizer, label2id)\n\n        # 5. Create data loader\n        test_loader = DataLoader(\n            test_tokenized,\n            batch_size=Config.batch_size,\n            shuffle=False\n        )\n        print(f\"Test batches: {len(test_loader)}\")\n\n        # 6. Initialize LoRA model\n        print(\"\\nInitializing LoRA model...\")\n        model = LoRAHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config\n        ).to(device)\n\n        # 7. Load model weights\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"LoRA model weights loaded successfully\")\n\n        # 8. Evaluate on test set\n        print(\"\\nEvaluating LoRA model on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list)\n\n        # 9. Generate confusion matrix\n        print(\"Generating confusion matrix with optimized visibility...\")\n        labels_idx = list(range(len(label_list)))\n        cm_path = plot_confusion_matrix(\n            test_metrics[\"all_labels\"],\n            test_metrics[\"all_preds\"],\n            labels=labels_idx,\n            label_names=label_list,  # Pass label names for axes\n            output_dir=Config.output_dir\n        )\n        print(f\"Confusion matrix saved to: {cm_path}\")\n\n        # Remove large arrays to save memory\n        del test_metrics[\"all_labels\"]\n        del test_metrics[\"all_preds\"]\n\n        # 10. Save test metrics\n        metrics_path = os.path.join(Config.output_dir, \"lora_test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            json.dump(test_metrics, f, indent=2)\n\n        print(f\"\\n{'='*30} LoRA TEST RESULTS {'='*30}\")\n        print(f\"Weighted F1:      {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Macro F1:         {test_metrics['macro_f1']:.4f}\")\n        print(f\"Accuracy:         {test_metrics['accuracy']:.4f}\")\n        print(f\"Weighted Precision: {test_metrics['weighted_precision']:.4f}\")\n        print(f\"Weighted Recall:    {test_metrics['weighted_recall']:.4f}\")\n        print(f\"Macro Precision:  {test_metrics['macro_precision']:.4f}\")\n        print(f\"Macro Recall:     {test_metrics['macro_recall']:.4f}\")\n        print(f\"Latency:          {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Evaluation time:  {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Metrics saved to: {metrics_path}\")\n\n        print(\"\\nPer-class Metrics:\")\n        for label in label_list:\n            print(f\"  {label}:\")\n            print(f\"    F1:       {test_metrics['per_label_f1'][label]:.4f}\")\n            print(f\"    Precision: {test_metrics['per_label_precision'][label]:.4f}\")\n            print(f\"    Recall:    {test_metrics['per_label_recall'][label]:.4f}\")\n\n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n\n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        with open(os.path.join(Config.output_dir, \"lora_error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n            f.write(traceback.format_exc())\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_lora_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN-Qlora\n","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"eval_qlora_model.py\n\nEvaluates a pre-trained QLoRA hierarchical model on the test set with optimized confusion matrix.\n\"\"\"\n\nfrom huggingface_hub import hf_hub_download\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader, Dataset as TorchDataset\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nfrom peft import PeftModel, PeftConfig\nimport random\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nclass Config:\n    # Will be updated from model's config\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    context_hidden_size = 256\n    max_num_sentences = 48\n    max_length = 128\n    dropout_rate = 0.4\n    gamma = 2.0\n    batch_size = 4  # Reduced for QLoRA memory constraints\n\n    # QLoRA specific parameters\n    lora_r = 128\n    lora_alpha = 64\n    lora_dropout = 0.15\n    lora_target_modules = [\"query\", \"value\", \"key\"]\n    qlora_quant_type = \"nf4\"\n    use_double_quant = True\n\n    # Paths and repo info\n    output_dir = \"./qlora_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-qlora-final\"  # Your QLoRA model repo\n\n\n# Load datasets\nsplits = {\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    return dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n\ndef tokenize_document(example, tokenizer, label2id):\n    \"\"\"Tokenize document with batch processing\"\"\"\n    sentences = example['text'][:Config.max_num_sentences]\n    labels = example['label'][:Config.max_num_sentences]\n    pad_len = Config.max_num_sentences - len(sentences)\n    \n    # Tokenize all sentences at once\n    tokenized = tokenizer(\n        sentences,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=Config.max_length,\n        return_tensors=\"pt\",\n        return_attention_mask=True\n    )\n    \n    # Pad to max sentences\n    pad_shape = (pad_len, Config.max_length)\n    input_ids = torch.cat([\n        tokenized[\"input_ids\"],\n        torch.full(pad_shape, tokenizer.pad_token_id, dtype=torch.long)\n    ])\n    attention_mask = torch.cat([\n        tokenized[\"attention_mask\"],\n        torch.zeros(pad_shape, dtype=torch.long)\n    ])\n    labels += [list(label2id.keys())[0]] * pad_len\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n    }\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.dropout = nn.Dropout(Config.dropout_rate)\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return self.dropout(x + self.position_emb(positions))\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling\"\"\"\n    def __init__(self, d_model, nhead=8, dim_feedforward=1024, dropout=0.2):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n            activation='gelu'\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            self.encoder_layer,\n            num_layers=2\n        )\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with residual connection\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.3):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, input_size*2)\n        self.linear2 = nn.Linear(input_size*2, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n        self.layer_norm = nn.LayerNorm(input_size*2)\n        self.residual_proj = nn.Linear(input_size, num_labels)\n\n    def forward(self, x):\n        residual = x\n        x = self.linear1(x)\n        x = self.layer_norm(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        return self.linear2(x) + self.residual_proj(residual)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        # Compute standard CRF loss\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n\n        # Apply focal loss transformation\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        # Apply class weights if provided\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]  # (batch_size, seq_len)\n            valid_counts = mask.sum(dim=1)  # (batch_size,)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass QLoRAHSLNModel(nn.Module):\n    \"\"\"Hierarchical model with QLoRA integration\"\"\"\n    def __init__(self, num_labels, model_config):\n        super().__init__()\n        # Update config from model_config\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        # Configure BitsAndBytes for 4-bit quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=Config.qlora_quant_type,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=Config.use_double_quant,\n        )\n\n        # Load base model with 4-bit quantization\n        base_bert = AutoModel.from_pretrained(\n            Config.bert_model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\"\n        )\n        \n        # Load QLoRA adapter\n        self.bert = PeftModel.from_pretrained(\n            base_bert,\n            Config.hf_repo_id,\n            device_map=\"auto\"\n        )\n\n        # Enhanced feature extraction\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.sent_projection = nn.Linear(\n            self.bert.config.hidden_size,\n            self.bert.config.hidden_size\n        )\n\n        # Context encoding\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n\n        # Emission layer\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n\n        # CRF layer with focal loss\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n        \n        # Get device from base model\n        self.device = next(self.bert.parameters()).device\n        print(f\"Model initialized on device: {self.device}\")\n        \n        # Move custom layers to same device as base model\n        self.sent_dropout = self.sent_dropout.to(self.device)\n        self.sent_layer_norm = self.sent_layer_norm.to(self.device)\n        self.sent_projection = self.sent_projection.to(self.device)\n        self.position_enc = self.position_enc.to(self.device)\n        self.context_encoder = self.context_encoder.to(self.device)\n        self.emission = self.emission.to(self.device)\n        self.crf = self.crf.to(self.device)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n        \n        # Move inputs to same device as model\n        input_ids = input_ids.to(self.device)\n        attention_mask = attention_mask.to(self.device)\n        if labels is not None:\n            labels = labels.to(self.device)\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Convert to float32 for custom layers\n        bert_out = bert_out.to(torch.float32)\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_projection(sent_emb)\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\ndef plot_confusion_matrix(y_true, y_pred, labels, label_names, output_dir):\n    \"\"\"Generate publication-quality confusion matrix with raw counts\"\"\"\n    # Compute confusion matrix with raw counts\n    cm = confusion_matrix(y_true, y_pred, labels=labels)\n    \n    # Create figure with optimized layout for multiple matrices\n    plt.figure(figsize=(8, 6), dpi=300)  # Slightly smaller size for fitting multiple\n    ax = plt.subplot()\n    \n    # Set font properties - make everything bold for better visibility\n    title_font = {'size': 12, 'weight': 'bold'}  # Smaller title size\n    label_font = {'size': 10, 'weight': 'bold'}  # Slightly smaller but bold\n    tick_font = {'size': 8, 'weight': 'bold'}    # Smaller tick size but bold\n    annot_font = {'size': 7, 'weight': 'bold'}   # Smaller annotation size but bold\n    \n    # Create heatmap with raw counts and bold annotations\n    sns.heatmap(\n        cm,\n        annot=True,\n        fmt='d',  # Integer format for raw counts\n        cmap=\"Blues\",\n        cbar=True,\n        cbar_kws={'label': 'Count', 'shrink': 0.75},  # Shrink colorbar for smaller plot\n        ax=ax,\n        annot_kws=annot_font  # Apply bold to cell values\n    )\n    \n    # Set labels and title with bold font\n    ax.set_title('Confusion Matrix (Raw Counts)', fontdict=title_font)\n    ax.set_xlabel('Predicted Label', fontdict=label_font)\n    ax.set_ylabel('True Label', fontdict=label_font)\n    \n    # Set tick labels with rotation\n    ax.set_xticks(np.arange(len(label_names)))\n    ax.set_xticklabels(label_names, rotation=45, ha=\"right\", fontdict=tick_font)\n    ax.set_yticks(np.arange(len(label_names)))\n    ax.set_yticklabels(label_names, rotation=0, fontdict=tick_font)\n    \n    # Adjust colorbar\n    cbar = ax.collections[0].colorbar\n    cbar.ax.tick_params(labelsize=8)  # Smaller colorbar labels\n    cbar.ax.set_ylabel('Count', fontsize=10, fontweight='bold')  # Bold colorbar label\n    \n    # Tight layout to maximize space\n    plt.tight_layout()\n    \n    # Save plot\n    plot_path = os.path.join(output_dir, \"confusion_matrix.png\")\n    plt.savefig(plot_path, bbox_inches='tight', pad_inches=0.05)  # Reduced padding\n    plt.close()\n    \n    return plot_path\n\nclass HierarchicalDataset(TorchDataset):\n    \"\"\"Custom dataset for hierarchical input\"\"\"\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        \n        # Ensure all values are tensors\n        input_ids = item[\"input_ids\"]\n        if not isinstance(input_ids, torch.Tensor):\n            input_ids = torch.tensor(input_ids, dtype=torch.long)\n        \n        attention_mask = item[\"attention_mask\"]\n        if not isinstance(attention_mask, torch.Tensor):\n            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n        \n        labels = item[\"labels\"]\n        if not isinstance(labels, torch.Tensor):\n            labels = torch.tensor(labels, dtype=torch.long)\n        \n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n\ndef collate_fn(batch):\n    \"\"\"Custom collate function for hierarchical input\"\"\"\n    # Convert all items to tensors and stack them\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n    \n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef convert_to_serializable(obj):\n    \"\"\"Recursively convert numpy types to native Python types for JSON serialization\"\"\"\n    if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64,\n                       np.uint8, np.uint16, np.uint32, np.uint64)):\n        return int(obj)\n    elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64)):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, dict):\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_serializable(item) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple(convert_to_serializable(item) for item in obj)\n    else:\n        return obj\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    model.eval()\n    all_preds, all_labels = [], []\n    total_time = 0\n    n_docs = 0\n    n_sentences = 0\n    eval_start = time.time()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            mask = attention_mask[:, :, 0] > 0\n\n            start = time.time()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            end = time.time()\n\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=mask)\n\n            for i in range(len(labels)):\n                seq_len = mask[i].sum().item()\n                all_preds.extend(preds[i][:seq_len])\n                all_labels.extend(labels[i][:seq_len].cpu().numpy().tolist())  # Convert to list of native ints\n\n            total_time += (end - start)\n            n_docs += input_ids.shape[0]\n            n_sentences += mask.sum().item()\n\n    eval_end = time.time()\n    eval_time = eval_end - eval_start\n\n    # Handle case where no samples were processed\n    if len(all_labels) == 0:\n        print(\"WARNING: No samples processed during evaluation!\")\n        return {\n            \"macro_f1\": 0,\n            \"weighted_f1\": 0,\n            \"accuracy\": 0,\n            \"macro_precision\": 0,\n            \"macro_recall\": 0,\n            \"weighted_precision\": 0,\n            \"weighted_recall\": 0,\n            \"latency_ms_per_doc\": 0,\n            \"latency_ms_per_sentence\": 0,\n            \"eval_time_seconds\": 0,\n            \"num_samples\": 0,\n            \"all_labels\": [],\n            \"all_preds\": []\n        }\n\n    labels_for_report = list(range(len(label_list)))\n    target_names = label_list\n\n    # Generate classification report\n    report = classification_report(\n        all_labels, all_preds,\n        labels=labels_for_report,\n        target_names=target_names,\n        output_dict=True,\n        zero_division=0\n    )\n\n    # Calculate additional metrics\n    macro_precision = precision_score(\n        all_labels, all_preds, average='macro', zero_division=0\n    )\n    macro_recall = recall_score(\n        all_labels, all_preds, average='macro', zero_division=0\n    )\n    weighted_precision = precision_score(\n        all_labels, all_preds, average='weighted', zero_division=0\n    )\n    weighted_recall = recall_score(\n        all_labels, all_preds, average='weighted', zero_division=0\n    )\n    per_label_precision = {\n        label: report[label]['precision']\n        for label in label_list\n    }\n    per_label_recall = {\n        label: report[label]['recall']\n        for label in label_list\n    }\n\n    macro_f1 = report['macro avg']['f1-score']\n    weighted_f1 = report['weighted avg']['f1-score']\n    accuracy = accuracy_score(all_labels, all_preds)\n    per_label_f1 = {\n        label: report[label]['f1-score']\n        for label in label_list\n    }\n\n    latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n    latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n    # Convert all metrics to serializable types\n    return {\n        \"macro_f1\": float(macro_f1),\n        \"weighted_f1\": float(weighted_f1),\n        \"accuracy\": float(accuracy),\n        \"per_label_f1\": {label: float(score) for label, score in per_label_f1.items()},\n        \"macro_precision\": float(macro_precision),\n        \"macro_recall\": float(macro_recall),\n        \"weighted_precision\": float(weighted_precision),\n        \"weighted_recall\": float(weighted_recall),\n        \"per_label_precision\": {label: float(score) for label, score in per_label_precision.items()},\n        \"per_label_recall\": {label: float(score) for label, score in per_label_recall.items()},\n        \"latency_ms_per_doc\": float(latency_doc),\n        \"latency_ms_per_sentence\": float(latency_sent),\n        \"eval_time_seconds\": float(eval_time),\n        \"num_samples\": int(n_docs),\n        \"all_labels\": all_labels,  # Already native ints\n        \"all_preds\": all_preds     # Already native ints\n    }\n\ndef evaluate_qlora_model():\n    \"\"\"Loads a pre-trained QLoRA model and evaluates it on the test set\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING QLoRA MODEL EVALUATION\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"Seed: {SEED}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts from Hugging Face Hub\n        print(\"Downloading QLoRA model artifacts from Hugging Face Hub...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n\n        # 2. Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n\n        label2id = saved_config['label2id']\n        id2label = saved_config['id2label']\n        model_config = saved_config['model_config']\n\n        # Convert id2label keys to integers\n        id2label = {int(k): v for k, v in id2label.items()}\n\n        # Create label list sorted by ID\n        label_list = [id2label[i] for i in range(len(id2label))]\n\n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        print(f\"Loaded configuration for QLoRA model: {Config.bert_model_name}\")\n        print(f\"QLoRA quantization: {Config.qlora_quant_type}\")\n        print(f\"Number of labels: {len(label_list)}\")\n\n        # 3. Preprocess test dataset\n        print(\"\\nPreprocessing test dataset...\")\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n\n        # 4. Tokenize test dataset using batch tokenization\n        print(\"Tokenizing test dataset with batch processing...\")\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n        test_tokenized = test_hier.map(\n            lambda x: tokenize_document(x, tokenizer, label2id),\n            batched=False\n        )\n\n        # 5. Create data loader with custom dataset and collate function\n        print(\"Creating data loader with custom collation...\")\n        test_dataset = HierarchicalDataset(test_tokenized)\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=Config.batch_size,\n            shuffle=False,\n            collate_fn=collate_fn\n        )\n        print(f\"Test batches: {len(test_loader)}\")\n\n        # 6. Initialize QLoRA model\n        print(\"\\nInitializing QLoRA model...\")\n        model = QLoRAHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config\n        )\n\n        # 7. Load custom layers weights\n        print(\"Loading custom layer weights...\")\n        # Get model's device for weight loading\n        model_device = next(model.parameters()).device\n        model.load_state_dict(\n            torch.load(model_path, map_location=model_device), \n            strict=False\n        )\n        print(\"QLoRA model weights loaded successfully\")\n\n        # 8. Evaluate on test set\n        print(\"\\nEvaluating QLoRA model on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list)\n\n        # 9. Generate confusion matrix with raw counts\n        print(\"Generating publication-quality confusion matrix with raw counts...\")\n        labels_idx = list(range(len(label_list)))\n        cm_path = plot_confusion_matrix(\n            test_metrics[\"all_labels\"],\n            test_metrics[\"all_preds\"],\n            labels=labels_idx,\n            label_names=label_list,\n            output_dir=Config.output_dir\n        )\n        print(f\"Confusion matrix saved to: {cm_path}\")\n\n        # 10. Save test metrics with proper serialization\n        metrics_path = os.path.join(Config.output_dir, \"qlora_test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            # Convert all metrics to serializable types\n            serializable_metrics = convert_to_serializable(test_metrics)\n            json.dump(serializable_metrics, f, indent=2)\n\n        print(f\"\\n{'='*30} QLoRA TEST RESULTS {'='*30}\")\n        print(f\"Weighted F1:      {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Macro F1:         {test_metrics['macro_f1']:.4f}\")\n        print(f\"Accuracy:         {test_metrics['accuracy']:.4f}\")\n        print(f\"Weighted Precision: {test_metrics['weighted_precision']:.4f}\")\n        print(f\"Weighted Recall:    {test_metrics['weighted_recall']:.4f}\")\n        print(f\"Macro Precision:  {test_metrics['macro_precision']:.4f}\")\n        print(f\"Macro Recall:     {test_metrics['macro_recall']:.4f}\")\n        print(f\"Latency:          {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Per-sentence:     {test_metrics['latency_ms_per_sentence']:.2f} ms/sent\")\n        print(f\"Evaluation time:  {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Metrics saved to: {metrics_path}\")\n\n        print(\"\\nPer-class Metrics:\")\n        for label in label_list:\n            print(f\"  {label}:\")\n            print(f\"    F1:       {test_metrics['per_label_f1'][label]:.4f}\")\n            print(f\"    Precision: {test_metrics['per_label_precision'][label]:.4f}\")\n            print(f\"    Recall:    {test_metrics['per_label_recall'][label]:.4f}\")\n\n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n\n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        with open(os.path.join(Config.output_dir, \"qlora_error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n            f.write(traceback.format_exc())\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_qlora_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN-Adalora","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import notebook_login","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"eval_adalora.py\n\nEvaluates AdaLoRA-enhanced hierarchical model on the test set with proper total_step handling.\n\"\"\"\n\nfrom huggingface_hub import hf_hub_download\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nfrom peft import AdaLoraConfig, PeftModel  # AdaLoRA-specific imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Set seed for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass Config:\n    # Will be updated from model's config\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    max_num_sentences = 48\n    max_length = 128\n    dropout_rate = 0.4\n    gamma = 2.0\n    batch_size = 4\n    output_dir = \"./adalora_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-adalora-final\"  # Update with your model repo\n    total_step = 10000  # Dummy value for evaluation\n\nimport pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import notebook_login\n\n# Load datasets\nsplits = {\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    dataset = dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n    return dataset\n\ndef tokenize_single_dataset(dataset, tokenizer, label2id):\n    \"\"\"Tokenize dataset for hierarchical input\"\"\"\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n\n    return dataset.map(tokenize_document)\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len):\n        super().__init__()\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return x + self.position_emb(positions)\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling\"\"\"\n    def __init__(self, d_model, nhead=8, dim_feedforward=1024, dropout=0.2):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n            activation='gelu'\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            self.encoder_layer, \n            num_layers=2\n        )\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with residual connection\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.3):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, input_size*2)\n        self.linear2 = nn.Linear(input_size*2, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n        self.layer_norm = nn.LayerNorm(input_size*2)\n        self.residual_proj = nn.Linear(input_size, num_labels)\n\n    def forward(self, x):\n        residual = x\n        x = self.linear1(x)\n        x = self.layer_norm(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        return self.linear2(x) + self.residual_proj(residual)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]\n            valid_counts = mask.sum(dim=1)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"AdaLoRA-enhanced hierarchical model with proper total_step handling\"\"\"\n    def __init__(self, num_labels, model_config, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Update config from model_config\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        # Base BERT model\n        base_bert = AutoModel.from_pretrained(Config.bert_model_name)\n        \n        # AdaLoRA configuration with total_step\n        adalora_config = AdaLoraConfig(\n            init_r=model_config.get('adalora_init_r', 64),\n            target_r=model_config.get('adalora_target_r', 4608),\n            beta1=model_config.get('adalora_beta1', 0.85),\n            beta2=model_config.get('adalora_beta2', 0.85),\n            tinit=model_config.get('adalora_tinit', 200),\n            tfinal=model_config.get('adalora_tfinal', 1000),\n            deltaT=model_config.get('adalora_deltaT', 10),\n            lora_alpha=model_config.get('lora_alpha', 64),\n            lora_dropout=model_config.get('lora_dropout', 0.15),\n            target_modules=model_config.get('lora_target_modules', [\"query\", \"value\", \"key\"]),\n            bias=\"none\",\n            total_step=Config.total_step  # Use the dummy value\n        )\n        \n        # Wrap BERT with AdaLoRA\n        self.bert = PeftModel(\n            base_bert, \n            adalora_config\n        )\n        \n        # Feature extraction\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.sent_projection = nn.Linear(\n            self.bert.config.hidden_size,\n            self.bert.config.hidden_size\n        )\n\n        # Context modeling\n        self.position_enc = PositionalEncoding(\n            self.bert.config.hidden_size,\n            max_len=Config.max_num_sentences\n        )\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n\n        # Emission layer\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n\n        # CRF layer with focal loss\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_projection(sent_emb)\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask,\n                class_weights=self.class_weights\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\nclass HierarchicalDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            \"input_ids\": item[\"input_ids\"],\n            \"attention_mask\": item[\"attention_mask\"],\n            \"labels\": item[\"labels\"]\n        }\n\ndef collate_fn(batch):\n    def ensure_tensor(x):\n        return torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n\n    input_ids = torch.stack([ensure_tensor(item[\"input_ids\"]) for item in batch])\n    attention_mask = torch.stack([ensure_tensor(item[\"attention_mask\"]) for item in batch])\n    labels = torch.stack([ensure_tensor(item[\"labels\"]) for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef create_single_loader(dataset):\n    return DataLoader(\n        HierarchicalDataset(dataset),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\ndef plot_confusion_matrix(cm, labels, output_path, title='Confusion Matrix'):\n    \"\"\"Plot confusion matrix with specified styling\"\"\"\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n                     xticklabels=labels, yticklabels=labels)\n    \n    # Bold labels and adjust font sizes\n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.set_xlabel('Predicted', fontsize=12, fontweight='bold')\n    ax.set_ylabel('Actual', fontsize=12, fontweight='bold')\n    \n    # Bold and rotate tick labels\n    plt.xticks(fontweight='bold', rotation=45, ha='right')\n    plt.yticks(fontweight='bold')\n    \n    # Make values more visible\n    for t in ax.texts:\n        t.set_text(t.get_text() + \" \")\n        t.set_fontweight('bold')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef convert_to_serializable(obj):\n    \"\"\"Convert numpy data types to Python-native types for JSON serialization\"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, dict):\n        return {k: convert_to_serializable(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [convert_to_serializable(item) for item in obj]\n    else:\n        return obj\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    try:\n        model.eval()\n        all_preds, all_labels = [], []\n        total_time = 0\n        n_docs = 0\n        n_sentences = 0\n        eval_start = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                mask = attention_mask[:, :, 0] > 0\n\n                start = time.time()\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                end = time.time()\n\n                emissions = outputs[\"emissions\"]\n                preds = model.crf.decode(emissions, mask=mask)\n\n                for i in range(len(labels)):\n                    seq_len = mask[i].sum().item()\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n                total_time += (end - start)\n                n_docs += input_ids.shape[0]\n                n_sentences += mask.sum().item()\n\n        eval_end = time.time()\n        eval_time = eval_end - eval_start\n\n        labels_for_report = list(range(len(label_list)))\n        target_names = label_list\n\n        report = classification_report(\n            all_labels, all_preds,\n            labels=labels_for_report,\n            target_names=target_names,\n            output_dict=True,\n            zero_division=0\n        )\n\n        # Calculate confusion matrix\n        cm = confusion_matrix(all_labels, all_preds, labels=labels_for_report)\n        \n        # Extract metrics\n        macro_f1 = report['macro avg']['f1-score']\n        weighted_f1 = report['weighted avg']['f1-score']\n        accuracy = accuracy_score(all_labels, all_preds)\n        \n        # Extract precision and recall metrics\n        macro_precision = report['macro avg']['precision']\n        macro_recall = report['macro avg']['recall']\n        weighted_precision = report['weighted avg']['precision']\n        weighted_recall = report['weighted avg']['recall']\n        \n        per_label_metrics = {}\n        for label in label_list:\n            per_label_metrics[label] = {\n                'f1': report[label]['f1-score'],\n                'precision': report[label]['precision'],\n                'recall': report[label]['recall']\n            }\n\n        latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n        latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n        return {\n            \"macro_f1\": macro_f1,\n            \"macro_precision\": macro_precision,\n            \"macro_recall\": macro_recall,\n            \"weighted_f1\": weighted_f1,\n            \"weighted_precision\": weighted_precision,\n            \"weighted_recall\": weighted_recall,\n            \"accuracy\": accuracy,\n            \"per_label_metrics\": per_label_metrics,\n            \"confusion_matrix\": cm.tolist(),\n            \"latency_ms_per_doc\": latency_doc,\n            \"latency_ms_per_sentence\": latency_sent,\n            \"eval_time_seconds\": eval_time,\n            \"num_samples\": n_docs\n        }\n\n    except Exception as e:\n        print(f\"Evaluation failed: {str(e)}\")\n        raise\n\ndef evaluate_test_set():\n    \"\"\"Loads AdaLoRA-enhanced model and evaluates on test set with total_step fix\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"ADALORA TEST SET EVALUATION\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"Seed: {SEED}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts\n        print(\"Downloading model artifacts...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n\n        # Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n\n        label2id = saved_config['label2id']\n        id2label = saved_config['id2label']\n        model_config = saved_config['model_config']\n\n        # Convert id2label keys to integers\n        id2label = {int(k): v for k, v in id2label.items()}\n\n        # Create sorted label list\n        label_list = [id2label[i] for i in range(len(id2label))]\n\n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        print(f\"Loaded configuration for AdaLoRA model\")\n        print(f\"Number of labels: {len(label_list)}\")\n        print(f\"AdaLoRA target_r: {model_config.get('adalora_target_r', 4608)}\")\n\n        # 2. Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n        # 3. Preprocess test dataset\n        print(\"\\nPreprocessing test dataset...\")\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n\n        # 4. Tokenize test dataset\n        print(\"Tokenizing test dataset...\")\n        test_tokenized = tokenize_single_dataset(test_hier, tokenizer, label2id)\n\n        # 5. Create data loader\n        test_loader = create_single_loader(test_tokenized)\n        print(f\"Test batches: {len(test_loader)}\")\n\n        # 6. Initialize AdaLoRA-enhanced model\n        print(\"\\nInitializing AdaLoRA model...\")\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config\n        ).to(device)\n\n        # 7. Load model weights\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"AdaLoRA model weights loaded successfully\")\n\n        # 8. Evaluate on test set\n        print(\"\\nEvaluating on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list)\n\n        # 9. Save test metrics\n        metrics_path = os.path.join(Config.output_dir, \"test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            # Convert metrics to serializable format\n            serializable_metrics = convert_to_serializable(test_metrics)\n            json.dump(serializable_metrics, f, indent=2)\n\n        # 10. Plot confusion matrix\n        cm_path = os.path.join(Config.output_dir, \"confusion_matrix.png\")\n        cm = np.array(test_metrics[\"confusion_matrix\"])\n        plot_confusion_matrix(cm, label_list, cm_path, \"AdaLoRA Model Confusion Matrix\")\n        \n        print(f\"\\n{'='*30} ADALORA TEST RESULTS {'='*30}\")\n        print(f\"Macro F1:     {test_metrics['macro_f1']:.4f}\")\n        print(f\"Macro Prec:   {test_metrics['macro_precision']:.4f}\")\n        print(f\"Macro Recall: {test_metrics['macro_recall']:.4f}\")\n        print(f\"Weighted F1:  {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Accuracy:     {test_metrics['accuracy']:.4f}\")\n        print(f\"Latency:      {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Evaluation time: {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Metrics saved to: {metrics_path}\")\n        print(f\"Confusion matrix saved to: {cm_path}\")\n\n        print(\"\\nPer-class Metrics:\")\n        for label, metrics in test_metrics['per_label_metrics'].items():\n            print(f\"  {label}:\")\n            print(f\"    F1:       {metrics['f1']:.4f}\")\n            print(f\"    Precision: {metrics['precision']:.4f}\")\n            print(f\"    Recall:    {metrics['recall']:.4f}\")\n\n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n\n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"ADALORA EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_test_set()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN-FSSA","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"eval_fssa_model.py\n\nEvaluation script for FSSA-based hierarchical legal model.\n\"\"\"\n\nfrom huggingface_hub import notebook_login, hf_hub_download\nimport torch.nn.functional as F\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport numpy as np\nimport math\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass Config:\n    # Defaults - will be overridden by saved config\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    lstm_hidden_size = 200\n    context_hidden_size = 200\n    max_num_sentences = 32\n    max_length = 64\n    dropout_rate = 0.4\n    gamma = 2.0\n    \n    # FSSA parameters\n    fssa_linear_rank = 1\n    fssa_emb_rank = 1\n    fssa_linear_sparsity = 0.99\n    fssa_emb_sparsity = 0.995\n    fssa_block_size = 32\n    \n    # Size reduction parameters\n    context_intermediate_size = 380\n    emission_hidden_size = 64\n    \n    # Evaluation settings\n    batch_size = 4\n    output_dir = \"./fssa_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-improved-fssa\"\n\n# =============================================\n# Model Components (copied from training script)\n# =============================================\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return x + self.position_emb(positions)\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling with reduced FFN size\"\"\"\n    def __init__(self, d_model, nhead=4, dim_feedforward=Config.context_intermediate_size, dropout=0.1):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Emission layer with reduced hidden size\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.2):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_size, Config.emission_hidden_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(Config.emission_hidden_size, num_labels)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n        \n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]\n            valid_counts = mask.sum(dim=1)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass FSSALayer(nn.Module):\n    \"\"\"Factorized Structured Sparse Adaptation layer\"\"\"\n    def __init__(self, original_layer, rank=Config.fssa_linear_rank,\n                 sparsity=Config.fssa_linear_sparsity, block_size=Config.fssa_block_size):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.sparsity = sparsity\n        self.block_size = block_size\n\n        # Freeze original parameters\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n\n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n\n        # Factorized adaptation parameters\n        self.A = nn.Parameter(torch.zeros(rank, in_features))\n        self.B = nn.Parameter(torch.zeros(out_features, rank))\n\n        # Structured sparsity mask\n        self.mask = self.create_sparsity_mask(out_features, in_features)\n        self.init_parameters()\n\n    def create_sparsity_mask(self, rows, cols):\n        row_blocks = (rows + self.block_size - 1) // self.block_size\n        col_blocks = (cols + self.block_size - 1) // self.block_size\n        num_blocks = row_blocks * col_blocks\n        num_active = int(num_blocks * (1 - self.sparsity))\n        active_blocks = random.sample(range(num_blocks), num_active)\n\n        mask = torch.zeros(rows, cols)\n        for block_idx in active_blocks:\n            i = block_idx // col_blocks\n            j = block_idx % col_blocks\n            row_start = i * self.block_size\n            col_start = j * self.block_size\n            row_end = min(row_start + self.block_size, rows)\n            col_end = min(col_start + self.block_size, cols)\n            mask[row_start:row_end, col_start:col_end] = 1\n\n        return mask\n\n    def init_parameters(self):\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.B, a=math.sqrt(5))\n\n    def forward(self, x):\n        base_output = self.original_layer(x)\n        adapted = (self.B @ self.A) * self.mask.to(self.B.device)\n        delta_output = F.linear(x, adapted)\n        return base_output + delta_output\n\nclass FSSAEmbedding(nn.Module):\n    \"\"\"FSSA for embeddings\"\"\"\n    def __init__(self, original_embedding, rank=Config.fssa_emb_rank,\n                 sparsity=Config.fssa_emb_sparsity):\n        super().__init__()\n        self.original_embedding = original_embedding\n        self.rank = rank\n        self.sparsity = sparsity\n\n        # Freeze original parameters\n        for param in self.original_embedding.parameters():\n            param.requires_grad = False\n\n        num_embeddings = original_embedding.num_embeddings\n        embedding_dim = original_embedding.embedding_dim\n\n        # Factorized adaptation\n        self.U = nn.Parameter(torch.zeros(num_embeddings, rank))\n        self.V = nn.Parameter(torch.zeros(rank, embedding_dim))\n        self.mask = (torch.rand(num_embeddings, rank) > sparsity).float()\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.U, mean=0, std=0.02)\n        nn.init.normal_(self.V, mean=0, std=0.02)\n\n    def forward(self, input_ids):\n        base_embeds = self.original_embedding(input_ids)\n        adapted = (self.U * self.mask.to(self.U.device)) @ self.V\n        delta_embeds = F.embedding(input_ids, adapted)\n        return base_embeds + delta_embeds\n\ndef apply_fssa(model):\n    \"\"\"Apply FSSA to model layers\"\"\"\n    # Apply to embeddings\n    if hasattr(model, 'embeddings'):\n        model.embeddings.word_embeddings = FSSAEmbedding(\n            model.embeddings.word_embeddings\n        )\n\n    # Apply to transformer layers\n    for layer in model.encoder.layer:\n        layer.attention.self.query = FSSALayer(layer.attention.self.query)\n        layer.attention.self.key = FSSALayer(layer.attention.self.key)\n        layer.attention.self.value = FSSALayer(layer.attention.self.value)\n        layer.attention.output.dense = FSSALayer(layer.attention.output.dense)\n        layer.intermediate.dense = FSSALayer(layer.intermediate.dense)\n        layer.output.dense = FSSALayer(layer.output.dense)\n\n    return model\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"FSSA-enhanced hierarchical model\"\"\"\n    def __init__(self, num_labels, model_config, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Load base BERT\n        self.bert = AutoModel.from_pretrained(model_config['bert_model_name'])\n        \n        # Apply FSSA modifications\n        self.bert = apply_fssa(self.bert)\n        \n        # Rest of the model\n        self.sent_dropout = nn.Dropout(model_config['dropout_rate'])\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n        self.crf = FocalCRF(num_labels, gamma=model_config['gamma'])\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask,\n                class_weights=self.class_weights\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\n# ===================================\n# Dataset Preparation and Evaluation\n# ===================================\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    dataset = dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n    return dataset\n\ndef tokenize_single_dataset(dataset, tokenizer, label2id):\n    \"\"\"Tokenize dataset for hierarchical input\"\"\"\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n\n    return dataset.map(tokenize_document)\n\nclass HierarchicalDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            \"input_ids\": item[\"input_ids\"],\n            \"attention_mask\": item[\"attention_mask\"],\n            \"labels\": item[\"labels\"]\n        }\n\ndef collate_fn(batch):\n    def ensure_tensor(x):\n        return torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n\n    input_ids = torch.stack([ensure_tensor(item[\"input_ids\"]) for item in batch])\n    attention_mask = torch.stack([ensure_tensor(item[\"attention_mask\"]) for item in batch])\n    labels = torch.stack([ensure_tensor(item[\"labels\"]) for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef create_single_loader(dataset):\n    return DataLoader(\n        HierarchicalDataset(dataset),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\ndef plot_confusion_matrix(cm, labels, output_path, title=\"Confusion Matrix\"):\n    \"\"\"Plot confusion matrix with specified formatting\"\"\"\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(\n        cm, \n        annot=True, \n        fmt='d', \n        cmap='Blues', \n        cbar=True,\n        annot_kws={\"fontsize\": 12, \"fontweight\": \"bold\"}\n    )\n    \n    ax.set_xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n    ax.set_ylabel('True Labels', fontsize=14, fontweight='bold')\n    ax.set_title(title, fontsize=16, fontweight='bold')\n    \n    # Set tick labels with bold font\n    ax.set_xticklabels(\n        labels, \n        rotation=45, \n        ha='right', \n        fontsize=12, \n        fontweight='bold'\n    )\n    ax.set_yticklabels(\n        labels, \n        rotation=0, \n        fontsize=12, \n        fontweight='bold'\n    )\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    model.eval()\n    all_preds, all_labels = [], []\n    total_time = 0\n    n_docs = 0\n    n_sentences = 0\n    eval_start = time.time()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            mask = attention_mask[:, :, 0] > 0\n\n            start = time.time()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            end = time.time()\n\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=mask)\n\n            for i in range(len(labels)):\n                seq_len = mask[i].sum().item()\n                all_preds.extend(preds[i][:seq_len])\n                all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n            total_time += (end - start)\n            n_docs += input_ids.shape[0]\n            n_sentences += mask.sum().item()\n\n    eval_end = time.time()\n    eval_time = eval_end - eval_start\n\n    labels_for_report = list(range(len(label_list)))\n    target_names = label_list\n\n    # Compute classification metrics\n    report = classification_report(\n        all_labels, all_preds,\n        labels=labels_for_report,\n        target_names=target_names,\n        output_dict=True,\n        zero_division=0\n    )\n    \n    # Compute precision and recall\n    precision = precision_score(\n        all_labels, all_preds, \n        labels=labels_for_report, \n        average='weighted', \n        zero_division=0\n    )\n    recall = recall_score(\n        all_labels, all_preds, \n        labels=labels_for_report, \n        average='weighted', \n        zero_division=0\n    )\n    \n    # Compute per-class precision and recall\n    per_label_precision = {\n        label: report[label]['precision'] \n        for label in label_list\n    }\n    per_label_recall = {\n        label: report[label]['recall'] \n        for label in label_list\n    }\n\n    macro_f1 = report['macro avg']['f1-score']\n    weighted_f1 = report['weighted avg']['f1-score']\n    accuracy = accuracy_score(all_labels, all_preds)\n    per_label_f1 = {\n        label: report[label]['f1-score']\n        for label in label_list\n    }\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(all_labels, all_preds, labels=labels_for_report)\n\n    latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n    latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"macro_f1\": macro_f1,\n        \"weighted_f1\": weighted_f1,\n        \"accuracy\": accuracy,\n        \"per_label_f1\": per_label_f1,\n        \"per_label_precision\": per_label_precision,\n        \"per_label_recall\": per_label_recall,\n        \"confusion_matrix\": cm.tolist(),\n        \"latency_ms_per_doc\": latency_doc,\n        \"latency_ms_per_sentence\": latency_sent,\n        \"eval_time_seconds\": eval_time,\n        \"num_samples\": n_docs\n    }\n\ndef evaluate_test_set():\n    \"\"\"Loads FSSA model and evaluates on test set\"\"\"\n    try:\n        # Set seeds again for double safety\n        random.seed(SEED)\n        np.random.seed(SEED)\n        torch.manual_seed(SEED)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(SEED)\n        \n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING FSSA MODEL EVALUATION\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"Random Seed: {SEED}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts\n        print(\"Downloading model artifacts from Hugging Face Hub...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n\n        # Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n\n        label2id = saved_config['label2id']\n        id2label = {int(k): v for k, v in saved_config['id2label'].items()}\n        model_config = saved_config['model_config']\n\n        # Create label list sorted by ID\n        label_list = [id2label[i] for i in range(len(id2label))]\n\n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        print(f\"Loaded configuration for model: {Config.bert_model_name}\")\n        print(f\"Number of labels: {len(label_list)}\")\n\n        # 2. Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n        # 3. Preprocess test dataset\n        import pandas as pd\n        from datasets import load_dataset, Dataset\n        \n        print(\"\\nPreprocessing test dataset...\")\n        splits = {'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'}\n        test_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\n        test_ds = Dataset.from_pandas(test_df)\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n\n        # 4. Tokenize test dataset\n        print(\"Tokenizing test dataset...\")\n        test_tokenized = tokenize_single_dataset(test_hier, tokenizer, label2id)\n\n        # 5. Create data loader\n        test_loader = create_single_loader(test_tokenized)\n        print(f\"Test batches: {len(test_loader)}\")\n\n        # 6. Initialize FSSA model\n        print(\"\\nInitializing FSSA model...\")\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config,\n            class_weights=None\n        ).to(device)\n\n        # 7. Load model weights\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"FSSA model weights loaded successfully\")\n\n        # 8. Evaluate on test set\n        print(\"\\nEvaluating on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list)\n\n        # 9. Save test metrics and plot confusion matrix\n        metrics_path = os.path.join(Config.output_dir, \"fssa_test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            json.dump(test_metrics, f, indent=2)\n        \n        # Plot and save confusion matrix\n        cm_path = os.path.join(Config.output_dir, \"fssa_confusion_matrix.png\")\n        cm = np.array(test_metrics[\"confusion_matrix\"])\n        plot_confusion_matrix(cm, label_list, cm_path)\n        \n        print(f\"\\n{'='*30} FSSA TEST RESULTS {'='*30}\")\n        print(f\"Weighted Precision: {test_metrics['precision']:.4f}\")\n        print(f\"Weighted Recall:    {test_metrics['recall']:.4f}\")\n        print(f\"Weighted F1:        {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Macro F1:           {test_metrics['macro_f1']:.4f}\")\n        print(f\"Accuracy:           {test_metrics['accuracy']:.4f}\")\n        print(f\"Latency:            {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Evaluation time:    {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Confusion matrix saved to: {cm_path}\")\n        print(f\"Metrics saved to:   {metrics_path}\")\n\n        print(\"\\nPer-class Metrics:\")\n        for label in label_list:\n            print(f\"  {label}:\")\n            print(f\"    Precision: {test_metrics['per_label_precision'][label]:.4f}\")\n            print(f\"    Recall:    {test_metrics['per_label_recall'][label]:.4f}\")\n            print(f\"    F1:        {test_metrics['per_label_f1'][label]:.4f}\")\n\n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n\n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"FSSA EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        with open(os.path.join(Config.output_dir, \"fssa_error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_test_set()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN-QloraFSSA\n","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"eval_qlora_fssa.py\n\nEvaluates a pre-trained QLora-FSSA model on the test set and saves metrics to JSON.\n\"\"\"\n\nfrom huggingface_hub import hf_hub_download\nimport torch.nn.functional as F\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport numpy as np\nimport math\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Set seed for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass Config:\n    # Default values - will be overwritten by saved config\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    max_num_sentences = 32\n    max_length = 64\n    dropout_rate = 0.4\n    gamma = 2.0\n    batch_size = 4\n    output_dir = \"./qlora_fssa_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-improved-fssa-qlora\"\n    \n    # FSSA parameters\n    fssa_linear_rank = 1\n    fssa_emb_rank = 1\n    fssa_linear_sparsity = 0.99\n    fssa_emb_sparsity = 0.995\n    fssa_block_size = 32\n    \n    # QLoRA parameters\n    qlora_r = 8\n    qlora_alpha = 32\n    qlora_dropout = 0.05\n    qlora_target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n    qlora_compute_dtype = torch.bfloat16\n    \n    # Size reduction parameters\n    context_intermediate_size = 380\n    emission_hidden_size = 64\n\n# Load datasets\nsplits = {\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    dataset = dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n    return dataset\n\ndef tokenize_single_dataset(dataset, tokenizer, label2id):\n    \"\"\"Tokenize dataset for hierarchical input\"\"\"\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n\n    return dataset.map(tokenize_document)\n\n# ====================== MODEL ARCHITECTURE ======================\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return x + self.position_emb(positions)\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling with reduced FFN size\"\"\"\n    def __init__(self, d_model, nhead=4, dim_feedforward=Config.context_intermediate_size, dropout=0.1):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Emission layer with reduced hidden size\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.2):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_size, Config.emission_hidden_size),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(Config.emission_hidden_size, num_labels)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n        \n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]\n            valid_counts = mask.sum(dim=1)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass FSSALayer(nn.Module):\n    \"\"\"Factorized Structured Sparse Adaptation layer\"\"\"\n    def __init__(self, original_layer, rank=Config.fssa_linear_rank,\n                 sparsity=Config.fssa_linear_sparsity, block_size=Config.fssa_block_size):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.sparsity = sparsity\n        self.block_size = block_size\n\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n\n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n\n        self.A = nn.Parameter(torch.zeros(rank, in_features))\n        self.B = nn.Parameter(torch.zeros(out_features, rank))\n        self.mask = self.create_sparsity_mask(out_features, in_features)\n        self.init_parameters()\n\n    def create_sparsity_mask(self, rows, cols):\n        row_blocks = (rows + self.block_size - 1) // self.block_size\n        col_blocks = (cols + self.block_size - 1) // self.block_size\n        num_blocks = row_blocks * col_blocks\n        num_active = int(num_blocks * (1 - self.sparsity))\n        active_blocks = random.sample(range(num_blocks), num_active)\n\n        mask = torch.zeros(rows, cols)\n        for block_idx in active_blocks:\n            i = block_idx // col_blocks\n            j = block_idx % col_blocks\n            row_start = i * self.block_size\n            col_start = j * self.block_size\n            row_end = min(row_start + self.block_size, rows)\n            col_end = min(col_start + self.block_size, cols)\n            mask[row_start:row_end, col_start:col_end] = 1\n        return mask\n\n    def init_parameters(self):\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.B, a=math.sqrt(5))\n\n    def forward(self, x):\n        base_output = self.original_layer(x)\n        adapted = (self.B @ self.A) * self.mask.to(self.B.device)\n        delta_output = F.linear(x, adapted)\n        return base_output + delta_output\n\nclass FSSAEmbedding(nn.Module):\n    \"\"\"Factorized Structured Sparse Adaptation for embeddings\"\"\"\n    def __init__(self, original_embedding, rank=Config.fssa_emb_rank,\n                 sparsity=Config.fssa_emb_sparsity):\n        super().__init__()\n        self.original_embedding = original_embedding\n        self.rank = rank\n        self.sparsity = sparsity\n\n        for param in self.original_embedding.parameters():\n            param.requires_grad = False\n\n        num_embeddings = original_embedding.num_embeddings\n        embedding_dim = original_embedding.embedding_dim\n\n        self.U = nn.Parameter(torch.zeros(num_embeddings, rank))\n        self.V = nn.Parameter(torch.zeros(rank, embedding_dim))\n        self.mask = (torch.rand(num_embeddings, rank) > sparsity).float()\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.U, mean=0, std=0.02)\n        nn.init.normal_(self.V, mean=0, std=0.02)\n\n    def forward(self, input_ids):\n        base_embeds = self.original_embedding(input_ids)\n        adapted = (self.U * self.mask.to(self.U.device)) @ self.V\n        delta_embeds = F.embedding(input_ids, adapted)\n        return base_embeds + delta_embeds\n\ndef apply_fssa_to_hierarchical(model):\n    \"\"\"Apply FSSA only to hierarchical components\"\"\"\n    model.position_enc.position_emb = FSSAEmbedding(model.position_enc.position_emb)\n    \n    for name, module in model.context_encoder.named_children():\n        if isinstance(module, nn.Linear):\n            setattr(model.context_encoder, name, FSSALayer(module))\n        else:\n            for sub_name, sub_module in module.named_children():\n                if isinstance(sub_module, nn.Linear):\n                    setattr(module, sub_name, FSSALayer(sub_module))\n    \n    for i, layer in enumerate(model.emission.mlp):\n        if isinstance(layer, nn.Linear):\n            model.emission.mlp[i] = FSSALayer(layer)\n            \n    return model\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"Hybrid QLoRA (BERT) + FSSA (Hierarchical) Model\"\"\"\n    def __init__(self, num_labels, model_config, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n        \n        # Load configuration overrides\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        # Configure 4-bit quantization\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=Config.qlora_compute_dtype\n        )\n\n        # Load BERT with QLoRA\n        self.bert = AutoModel.from_pretrained(\n            Config.bert_model_name,\n            quantization_config=bnb_config\n        )\n        self.bert = prepare_model_for_kbit_training(self.bert)\n        \n        # Apply QLoRA adapters\n        lora_config = LoraConfig(\n            r=Config.qlora_r,\n            lora_alpha=Config.qlora_alpha,\n            target_modules=Config.qlora_target_modules,\n            lora_dropout=Config.qlora_dropout,\n            bias=\"none\",\n            task_type=\"FEATURE_EXTRACTION\"\n        )\n        self.bert = get_peft_model(self.bert, lora_config)\n\n        # Hierarchical components\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n\n        # Apply FSSA to hierarchical components\n        self = apply_fssa_to_hierarchical(self)\n\n        # CRF layer\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask,\n                class_weights=self.class_weights\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n# ====================== END MODEL ARCHITECTURE ======================\n\nclass HierarchicalDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            \"input_ids\": item[\"input_ids\"],\n            \"attention_mask\": item[\"attention_mask\"],\n            \"labels\": item[\"labels\"]\n        }\n\ndef collate_fn(batch):\n    def ensure_tensor(x):\n        return torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n\n    input_ids = torch.stack([ensure_tensor(item[\"input_ids\"]) for item in batch])\n    attention_mask = torch.stack([ensure_tensor(item[\"attention_mask\"]) for item in batch])\n    labels = torch.stack([ensure_tensor(item[\"labels\"]) for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef create_single_loader(dataset):\n    return DataLoader(\n        HierarchicalDataset(dataset),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    try:\n        model.eval()\n        all_preds, all_labels = [], []\n        total_time = 0\n        n_docs = 0\n        n_sentences = 0\n        eval_start = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                mask = attention_mask[:, :, 0] > 0\n\n                start = time.time()\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                end = time.time()\n\n                emissions = outputs[\"emissions\"]\n                preds = model.crf.decode(emissions, mask=mask)\n\n                for i in range(len(labels)):\n                    seq_len = mask[i].sum().item()\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n                total_time += (end - start)\n                n_docs += input_ids.shape[0]\n                n_sentences += mask.sum().item()\n\n        eval_end = time.time()\n        eval_time = eval_end - eval_start\n\n        labels_for_report = list(range(len(label_list)))\n        target_names = label_list\n\n        report = classification_report(\n            all_labels, all_preds,\n            labels=labels_for_report,\n            target_names=target_names,\n            output_dict=True,\n            zero_division=0\n        )\n\n        # Calculate additional metrics\n        macro_f1 = report['macro avg']['f1-score']\n        weighted_f1 = report['weighted avg']['f1-score']\n        accuracy = accuracy_score(all_labels, all_preds)\n        \n        # Per-label metrics\n        per_label_metrics = {}\n        for label in label_list:\n            per_label_metrics[label] = {\n                'f1': report[label]['f1-score'],\n                'precision': report[label]['precision'],\n                'recall': report[label]['recall']\n            }\n        \n        # Confusion matrix (non-normalized)\n        cm = confusion_matrix(all_labels, all_preds, labels=labels_for_report)\n\n        latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n        latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n        return {\n            \"macro_f1\": macro_f1,\n            \"weighted_f1\": weighted_f1,\n            \"accuracy\": accuracy,\n            \"per_label_metrics\": per_label_metrics,\n            \"confusion_matrix\": cm,\n            \"latency_ms_per_doc\": latency_doc,\n            \"latency_ms_per_sentence\": latency_sent,\n            \"eval_time_seconds\": eval_time,\n            \"num_samples\": n_docs\n        }\n\n    except Exception as e:\n        print(f\"Evaluation failed: {str(e)}\")\n        raise\n\ndef plot_confusion_matrix(cm, classes, output_path):\n    \"\"\"Create and save confusion matrix plot\"\"\"\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                     cbar=True, annot_kws={\"size\": 12, \"weight\": \"bold\"})\n    \n    # Set labels and titles with bold font\n    ax.set_xlabel('Predicted Labels', fontsize=14, fontweight='bold')\n    ax.set_ylabel('True Labels', fontsize=14, fontweight='bold')\n    ax.set_title('Confusion Matrix', fontsize=16, fontweight='bold')\n    \n    # Set tick labels with bold font and smaller size for visibility\n    ax.set_xticklabels(classes, rotation=45, ha='right', \n                       fontsize=10, fontweight='bold')\n    ax.set_yticklabels(classes, rotation=0, \n                       fontsize=10, fontweight='bold')\n    \n    # Adjust colorbar font\n    cbar = ax.collections[0].colorbar\n    cbar.ax.tick_params(labelsize=10)\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef evaluate_test_set():\n    \"\"\"Loads a pre-trained model and evaluates it on the test set\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING TEST SET EVALUATION (QLora-FSSA)\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts from Hugging Face Hub\n        print(\"Downloading model artifacts from Hugging Face Hub...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n\n        # Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n\n        label2id = saved_config['label2id']\n        id2label = saved_config['id2label']\n        model_config = saved_config['model_config']\n\n        # Convert id2label keys to integers\n        id2label = {int(k): v for k, v in id2label.items()}\n\n        # Create label list sorted by ID\n        label_list = [id2label[i] for i in range(len(id2label))]\n\n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        print(f\"Loaded configuration for model: {Config.bert_model_name}\")\n        print(f\"Number of labels: {len(label_list)}\")\n\n        # 2. Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n        # 3. Preprocess test dataset\n        print(\"\\nPreprocessing test dataset...\")\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n\n        # 4. Tokenize test dataset\n        print(\"Tokenizing test dataset...\")\n        test_tokenized = tokenize_single_dataset(test_hier, tokenizer, label2id)\n\n        # 5. Create data loader\n        test_loader = create_single_loader(test_tokenized)\n        print(f\"Test batches: {len(test_loader)}\")\n\n        # 6. Initialize model\n        print(\"\\nInitializing model...\")\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config,\n            class_weights=None\n        ).to(device)\n\n        # 7. Load model weights\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"Model weights loaded successfully\")\n\n        # 8. Evaluate on test set\n        print(\"\\nEvaluating on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list)\n\n        # 9. Save confusion matrix plot\n        cm_path = os.path.join(Config.output_dir, \"confusion_matrix.png\")\n        plot_confusion_matrix(test_metrics[\"confusion_matrix\"], label_list, cm_path)\n        \n        # Convert confusion matrix to list for JSON serialization\n        test_metrics[\"confusion_matrix\"] = test_metrics[\"confusion_matrix\"].tolist()\n\n        # 10. Save test metrics\n        metrics_path = os.path.join(Config.output_dir, \"test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            json.dump(test_metrics, f, indent=2)\n\n        print(f\"\\n{'='*30} TEST RESULTS {'='*30}\")\n        print(f\"Weighted F1: {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Macro F1:    {test_metrics['macro_f1']:.4f}\")\n        print(f\"Accuracy:    {test_metrics['accuracy']:.4f}\")\n        print(f\"Latency:     {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Evaluation time: {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Metrics saved to: {metrics_path}\")\n        print(f\"Confusion matrix saved to: {cm_path}\")\n\n        print(\"\\nPer-class Metrics:\")\n        for label, metrics in test_metrics['per_label_metrics'].items():\n            print(f\"  {label}:\")\n            print(f\"    F1:       {metrics['f1']:.4f}\")\n            print(f\"    Precision: {metrics['precision']:.4f}\")\n            print(f\"    Recall:    {metrics['recall']:.4f}\")\n\n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n\n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n            f.write(\"\\n\\nTraceback:\\n\")\n            f.write(traceback.format_exc())\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_test_set()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN-Role_Routed","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"eval_role_routed.py\n\nEvaluates a pre-trained Role-Routed Adapter model on the test set and saves metrics to JSON.\n\"\"\"\n\nfrom huggingface_hub import hf_hub_download\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoConfig, AutoModel\nimport torch.nn as nn\nfrom torchcrf import CRF\nimport random\n\n# Set seeds for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\n\nclass Config:\n    # Role-Routed specific parameters\n    num_roles = 13\n    adapter_intermediate_size = 256\n    context_hidden_size = 256\n    max_num_sentences = 48\n    max_length = 96\n    dropout_rate = 0.4\n    gamma = 2.0\n    batch_size = 4\n    output_dir = \"./role_routed_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-role-routed\"\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n\n# Load datasets\nsplits = {\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    dataset = dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n    return dataset\n\ndef tokenize_single_dataset(dataset, tokenizer, label2id):\n    \"\"\"Tokenize dataset for hierarchical input\"\"\"\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n\n    return dataset.map(tokenize_document)\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings with dropout\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.dropout = nn.Dropout(Config.dropout_rate)\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return self.dropout(x + self.position_emb(positions))\n\nclass RoleRoutedAdapter(nn.Module):\n    \"\"\"Role-Routed Adapter with 13 parallel adapters for rhetorical roles\"\"\"\n    def __init__(self, config, role_count, intermediate_size=64):\n        super().__init__()\n        self.config = config\n        self.role_count = role_count\n        self.intermediate_size = intermediate_size\n\n        # 13 parallel adapters for each role\n        self.adapters = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(config.hidden_size, intermediate_size),\n                nn.GELU(),\n                nn.Linear(intermediate_size, config.hidden_size)\n            ) for _ in range(role_count)\n        ])\n\n        # Router to coordinate adapters\n        self.router = nn.Sequential(\n            nn.Linear(config.hidden_size, 256),\n            nn.Tanh(),\n            nn.Linear(256, role_count)\n        )\n\n        # Initialize weights\n        for adapter in self.adapters:\n            for layer in adapter:\n                if isinstance(layer, nn.Linear):\n                    layer.weight.data.normal_(mean=0.0, std=0.02)\n                    if layer.bias is not None:\n                        layer.bias.data.zero_()\n\n        for layer in self.router:\n            if isinstance(layer, nn.Linear):\n                layer.weight.data.normal_(mean=0.0, std=0.02)\n                if layer.bias is not None:\n                    layer.bias.data.zero_()\n\n    def forward(self, x):\n        batch_size, seq_len, hidden_size = x.shape\n        avg_emb = x.mean(dim=1)\n        router_logits = self.router(avg_emb)\n        routing_weights = torch.softmax(router_logits, dim=-1)\n\n        adapter_outputs = [adapter(x) for adapter in self.adapters]\n        adapter_outputs = torch.stack(adapter_outputs, dim=1)\n        weighted_output = torch.einsum('br,brsh->bsh', routing_weights, adapter_outputs)\n        return x + weighted_output\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Custom transformer layer with role-routed adapters\"\"\"\n    def __init__(self, d_model, nhead=8, dim_feedforward=1024, dropout=0.2, num_roles=Config.num_roles):\n        super().__init__()\n        self.d_model = d_model\n\n        # Multi-Head Attention\n        self.multihead_attn = nn.MultiheadAttention(\n            embed_dim=d_model,\n            num_heads=nhead,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.adapter1 = RoleRoutedAdapter(\n            config=AutoConfig.from_pretrained(Config.bert_model_name),\n            role_count=num_roles,\n            intermediate_size=Config.adapter_intermediate_size\n        )\n\n        # Feed Forward\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        self.adapter2 = RoleRoutedAdapter(\n            config=AutoConfig.from_pretrained(Config.bert_model_name),\n            role_count=num_roles,\n            intermediate_size=Config.adapter_intermediate_size\n        )\n\n    def forward(self, x):\n        attn_output, _ = self.multihead_attn(x, x, x)\n        x = self.norm1(x + attn_output)\n        x = self.adapter1(x)\n\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + ff_output)\n        x = self.adapter2(x)\n        return x\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with residual connection\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.3):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, input_size*2)\n        self.linear2 = nn.Linear(input_size*2, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n        self.layer_norm = nn.LayerNorm(input_size*2)\n        self.residual_proj = nn.Linear(input_size, num_labels)\n\n    def forward(self, x):\n        residual = x\n        x = self.linear1(x)\n        x = self.layer_norm(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        return self.linear2(x) + self.residual_proj(residual)\n\nclass FocalCRF(nn.Module):\n    \"\"\"Fixed CRF with focal loss\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]\n            valid_counts = mask.sum(dim=1)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"HSLN model with Role-Routed Adapters\"\"\"\n    def __init__(self, num_labels, model_config, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Update config from model_config\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        # Load base Legal-BERT\n        self.bert = AutoModel.from_pretrained(Config.bert_model_name)\n\n        # Feature extraction layers\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.sent_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n\n        # Context encoding\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(d_model=self.bert.config.hidden_size)\n\n        # Emission and CRF\n        self.emission = EmissionLayer(input_size=self.bert.config.hidden_size, num_labels=num_labels)\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(input_ids=flat_input_ids, attention_mask=flat_mask).last_hidden_state\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_projection(sent_emb)\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0\n\n        if labels is not None:\n            loss = self.crf(emissions, labels, mask=mask, class_weights=self.class_weights)\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\ndef collate_fn(batch):\n    \"\"\"Fixed collate function to handle tensor conversion\"\"\"\n    input_ids = []\n    attention_mask = []\n    labels = []\n    \n    for item in batch:\n        # Convert to tensors if they're not already\n        if not isinstance(item[\"input_ids\"], torch.Tensor):\n            input_ids.append(torch.tensor(item[\"input_ids\"], dtype=torch.long))\n        else:\n            input_ids.append(item[\"input_ids\"])\n            \n        if not isinstance(item[\"attention_mask\"], torch.Tensor):\n            attention_mask.append(torch.tensor(item[\"attention_mask\"], dtype=torch.long))\n        else:\n            attention_mask.append(item[\"attention_mask\"])\n            \n        if not isinstance(item[\"labels\"], torch.Tensor):\n            labels.append(torch.tensor(item[\"labels\"], dtype=torch.long))\n        else:\n            labels.append(item[\"labels\"])\n    \n    return {\n        \"input_ids\": torch.stack(input_ids),\n        \"attention_mask\": torch.stack(attention_mask),\n        \"labels\": torch.stack(labels)\n    }\n\ndef create_single_loader(dataset):\n    return DataLoader(\n        dataset,\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\ndef plot_confusion_matrix(y_true, y_pred, labels, output_path, title):\n    \"\"\"Plot non-normalized confusion matrix with bold labels\"\"\"\n    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(labels)))\n    \n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(\n        cm, \n        annot=True, \n        fmt=\"d\", \n        cmap=\"Blues\", \n        xticklabels=labels, \n        yticklabels=labels,\n        cbar_kws={'label': 'Count'}\n    )\n    \n    # Make labels bold and rotate\n    plt.xticks(fontsize=10, fontweight='bold', rotation=45, ha='right')\n    plt.yticks(fontsize=10, fontweight='bold')\n    \n    # Bold axis labels\n    plt.xlabel('Predicted Labels', fontsize=12, fontweight='bold')\n    plt.ylabel('True Labels', fontsize=12, fontweight='bold')\n    plt.title(title, fontsize=14, fontweight='bold')\n    \n    # Adjust layout to fit labels\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300)\n    plt.close()\n\ndef evaluate_metrics(model, dataloader, device, label_list, output_dir):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    try:\n        model.eval()\n        all_preds, all_labels = [], []\n        total_time = 0\n        n_docs = 0\n        n_sentences = 0\n        eval_start = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                mask = attention_mask[:, :, 0] > 0\n\n                start = time.time()\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                end = time.time()\n\n                emissions = outputs[\"emissions\"]\n                preds = model.crf.decode(emissions, mask=mask)\n\n                for i in range(len(labels)):\n                    seq_len = mask[i].sum().item()\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n                total_time += (end - start)\n                n_docs += input_ids.shape[0]\n                n_sentences += mask.sum().item()\n\n        eval_end = time.time()\n        eval_time = eval_end - eval_start\n\n        labels_for_report = list(range(len(label_list)))\n        target_names = label_list\n\n        report = classification_report(\n            all_labels, all_preds,\n            labels=labels_for_report,\n            target_names=target_names,\n            output_dict=True,\n            zero_division=0\n        )\n\n        # Generate confusion matrix\n        cm_path = os.path.join(output_dir, \"confusion_matrix.png\")\n        plot_confusion_matrix(\n            all_labels, \n            all_preds, \n            label_list, \n            cm_path,\n            \"Role-Routed Model Confusion Matrix\"\n        )\n\n        # Extract metrics\n        macro_f1 = report['macro avg']['f1-score']\n        weighted_f1 = report['weighted avg']['f1-score']\n        accuracy = accuracy_score(all_labels, all_preds)\n        \n        per_label_metrics = {}\n        for label in label_list:\n            per_label_metrics[label] = {\n                'f1': report[label]['f1-score'],\n                'precision': report[label]['precision'],\n                'recall': report[label]['recall'],\n                'support': report[label]['support']\n            }\n\n        latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n        latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n        return {\n            \"macro_f1\": macro_f1,\n            \"weighted_f1\": weighted_f1,\n            \"accuracy\": accuracy,\n            \"per_label_metrics\": per_label_metrics,\n            \"latency_ms_per_doc\": latency_doc,\n            \"latency_ms_per_sentence\": latency_sent,\n            \"eval_time_seconds\": eval_time,\n            \"num_samples\": n_docs,\n            \"confusion_matrix_path\": cm_path\n        }\n\n    except Exception as e:\n        print(f\"Evaluation failed: {str(e)}\")\n        raise\n\ndef evaluate_test_set():\n    \"\"\"Loads a pre-trained model and evaluates it on the test set\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING ROLE-ROUTED MODEL EVALUATION\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"Seed: {SEED}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts from Hugging Face Hub\n        print(\"Downloading model artifacts from Hugging Face Hub...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n\n        # Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n\n        label2id = saved_config['label2id']\n        id2label = saved_config['id2label']\n        model_config = saved_config['model_config']\n\n        # Convert id2label keys to integers\n        id2label = {int(k): v for k, v in id2label.items()}\n\n        # Create label list sorted by ID\n        label_list = [id2label[i] for i in range(len(id2label))]\n\n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n\n        print(f\"Loaded configuration for model: {Config.bert_model_name}\")\n        print(f\"Number of labels: {len(label_list)}\")\n        print(f\"Role-Routed Parameters: num_roles={Config.num_roles}, adapter_size={Config.adapter_intermediate_size}\")\n\n        # 2. Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n        # 3. Preprocess test dataset\n        print(\"\\nPreprocessing test dataset...\")\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n\n        # 4. Tokenize test dataset\n        print(\"Tokenizing test dataset...\")\n        test_tokenized = tokenize_single_dataset(test_hier, tokenizer, label2id)\n\n        # 5. Create data loader\n        test_loader = create_single_loader(test_tokenized)\n        print(f\"Test batches: {len(test_loader)}\")\n\n        # 6. Initialize model\n        print(\"\\nInitializing Role-Routed model...\")\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config,\n            class_weights=None\n        ).to(device)\n\n        # 7. Load model weights\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"Model weights loaded successfully\")\n\n        # 8. Evaluate on test set\n        print(\"\\nEvaluating on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list, Config.output_dir)\n\n        # 9. Save test metrics\n        metrics_path = os.path.join(Config.output_dir, \"test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            json.dump(test_metrics, f, indent=2)\n\n        print(f\"\\n{'='*30} TEST RESULTS {'='*30}\")\n        print(f\"Weighted F1: {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Macro F1:    {test_metrics['macro_f1']:.4f}\")\n        print(f\"Accuracy:    {test_metrics['accuracy']:.4f}\")\n        print(f\"Latency:     {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Evaluation time: {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Metrics saved to: {metrics_path}\")\n        print(f\"Confusion matrix saved to: {test_metrics['confusion_matrix_path']}\")\n\n        print(\"\\nPer-class Metrics:\")\n        for label, metrics in test_metrics['per_label_metrics'].items():\n            print(f\"  {label}:\")\n            print(f\"    F1:       {metrics['f1']:.4f}\")\n            print(f\"    Precision:{metrics['precision']:.4f}\")\n            print(f\"    Recall:   {metrics['recall']:.4f}\")\n            print(f\"    Support:  {metrics['support']}\")\n\n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n\n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_test_set()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LegalBERTHSLN-Baseline-Full_Finetuning","metadata":{}},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"test4_evaluate_test_set\n\nEvaluates a pre-trained model on the test set and saves metrics to JSON.\n\"\"\"\n\nfrom huggingface_hub import hf_hub_download, snapshot_download\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport psutil\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\n# Set seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass Config:\n    # These will be updated from the model's config\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    lstm_hidden_size = 200\n    context_hidden_size = 200\n    max_num_sentences = 32\n    max_length = 128\n    dropout_rate = 0.4\n    gamma = 2.0\n    batch_size = 4\n    output_dir = \"./test_evaluation_results\"\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-improved-augmentation\"\n\nimport pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import notebook_login\n\n# Load datasets\nsplits = {\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_spans_and_labels(example):\n    \"\"\"Extract spans and labels from example\"\"\"\n    spans = []\n    labels = []\n    if example.get('annotations') and len(example['annotations']) > 0:\n        if example['annotations'][0].get('result'):\n            for ann in example['annotations'][0]['result']:\n                if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                    spans.append(ann['value']['text'])\n                    labels.append(ann['value']['labels'][0])\n    return {'spans': spans, 'labels': labels}\n\ndef preprocess_single_dataset(dataset, label2id):\n    \"\"\"Preprocess dataset for evaluation\"\"\"\n    dataset = dataset.map(get_spans_and_labels)\n    dataset = dataset.filter(lambda x: len(x['spans']) > 0)\n    dataset = dataset.map(lambda x: {'text': x['spans'], 'label': x['labels']})\n    return dataset\n\ndef tokenize_single_dataset(dataset, tokenizer, label2id):\n    \"\"\"Tokenize dataset for hierarchical input\"\"\"\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n    \n    return dataset.map(tokenize_document)\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len):\n        super().__init__()\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return x + self.position_emb(positions)\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling\"\"\"\n    def __init__(self, d_model, nhead=4, dim_feedforward=512, dropout=0.1):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with MLP\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.2):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_size, input_size*2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(input_size*2, num_labels)\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        # Compute standard CRF loss\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n\n        # Apply focal loss transformation\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        # Apply class weights if provided\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]  # (batch_size, seq_len)\n            valid_counts = mask.sum(dim=1)  # (batch_size,)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"Enhanced hierarchical model with class imbalance handling\"\"\"\n    def __init__(self, num_labels, model_config, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Update config from model_config\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n        \n        # Sentence encoding\n        self.bert = AutoModel.from_pretrained(Config.bert_model_name)\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n\n        # Context encoding\n        self.position_enc = PositionalEncoding(\n            self.bert.config.hidden_size, \n            max_len=Config.max_num_sentences\n        )\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n\n        # Emission layer\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n\n        # CRF layer with focal loss\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask,\n                class_weights=self.class_weights\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\nclass HierarchicalDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            \"input_ids\": item[\"input_ids\"],\n            \"attention_mask\": item[\"attention_mask\"],\n            \"labels\": item[\"labels\"]\n        }\n\ndef collate_fn(batch):\n    def ensure_tensor(x):\n        return torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n\n    input_ids = torch.stack([ensure_tensor(item[\"input_ids\"]) for item in batch])\n    attention_mask = torch.stack([ensure_tensor(item[\"attention_mask\"]) for item in batch])\n    labels = torch.stack([ensure_tensor(item[\"labels\"]) for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef create_single_loader(dataset):\n    return DataLoader(\n        HierarchicalDataset(dataset),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n\ndef plot_confusion_matrix(cm, labels, output_dir):\n    \"\"\"Create and save a confusion matrix with specified formatting\"\"\"\n    plt.figure(figsize=(10, 8))\n    ax = sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n                     annot_kws={\"size\": 12, \"weight\": \"bold\"})\n    \n    # Set axis labels with bold font\n    ax.set_xlabel('Predicted labels', fontsize=14, fontweight='bold')\n    ax.set_ylabel('True labels', fontsize=14, fontweight='bold')\n    \n    # Set tick labels with bold font\n    ax.set_xticklabels(labels, rotation=45, ha=\"right\", fontsize=12, fontweight='bold')\n    ax.set_yticklabels(labels, rotation=0, fontsize=12, fontweight='bold')\n    \n    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n    plt.tight_layout()\n    \n    # Save the figure\n    plt.savefig(os.path.join(output_dir, \"confusion_matrix.png\"), dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef evaluate_metrics(model, dataloader, device, label_list, output_dir):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    try:\n        model.eval()\n        all_preds, all_labels = [], []\n        total_time = 0\n        n_docs = 0\n        n_sentences = 0\n        eval_start = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                mask = attention_mask[:, :, 0] > 0\n\n                start = time.time()\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                end = time.time()\n\n                emissions = outputs[\"emissions\"]\n                preds = model.crf.decode(emissions, mask=mask)\n\n                for i in range(len(labels)):\n                    seq_len = mask[i].sum().item()\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n                total_time += (end - start)\n                n_docs += input_ids.shape[0]\n                n_sentences += mask.sum().item()\n\n        eval_end = time.time()\n        eval_time = eval_end - eval_start\n\n        labels_for_report = list(range(len(label_list)))\n        target_names = label_list\n\n        report = classification_report(\n            all_labels, all_preds,\n            labels=labels_for_report,\n            target_names=target_names,\n            output_dict=True,\n            zero_division=0\n        )\n\n        # Generate confusion matrix\n        cm = confusion_matrix(all_labels, all_preds, labels=labels_for_report)\n        plot_confusion_matrix(cm, label_list, output_dir)\n\n        # Extract metrics\n        macro_f1 = report['macro avg']['f1-score']\n        weighted_f1 = report['weighted avg']['f1-score']\n        accuracy = accuracy_score(all_labels, all_preds)\n        macro_precision = report['macro avg']['precision']\n        macro_recall = report['macro avg']['recall']\n        weighted_precision = report['weighted avg']['precision']\n        weighted_recall = report['weighted avg']['recall']\n        \n        per_label_f1 = {}\n        per_label_precision = {}\n        per_label_recall = {}\n        for label in label_list:\n            per_label_f1[label] = report[label]['f1-score']\n            per_label_precision[label] = report[label]['precision']\n            per_label_recall[label] = report[label]['recall']\n\n        latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n        latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n        return {\n            \"macro_f1\": macro_f1,\n            \"weighted_f1\": weighted_f1,\n            \"accuracy\": accuracy,\n            \"macro_precision\": macro_precision,\n            \"macro_recall\": macro_recall,\n            \"weighted_precision\": weighted_precision,\n            \"weighted_recall\": weighted_recall,\n            \"per_label_f1\": per_label_f1,\n            \"per_label_precision\": per_label_precision,\n            \"per_label_recall\": per_label_recall,\n            \"latency_ms_per_doc\": latency_doc,\n            \"latency_ms_per_sentence\": latency_sent,\n            \"eval_time_seconds\": eval_time,\n            \"num_samples\": n_docs\n        }\n\n    except Exception as e:\n        print(f\"Evaluation failed: {str(e)}\")\n        raise\n\ndef evaluate_test_set():\n    \"\"\"Loads a pre-trained model and evaluates it on the test set\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING TEST SET EVALUATION\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Model Repo: {Config.hf_repo_id}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # 1. Download model artifacts from Hugging Face Hub\n        print(\"Downloading model artifacts from Hugging Face Hub...\")\n        config_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"config.json\"\n        )\n        model_path = hf_hub_download(\n            repo_id=Config.hf_repo_id,\n            filename=\"pytorch_model.bin\"\n        )\n        \n        # Load configuration\n        with open(config_path, 'r') as f:\n            saved_config = json.load(f)\n        \n        label2id = saved_config['label2id']\n        id2label = saved_config['id2label']\n        model_config = saved_config['model_config']\n        \n        # Convert id2label keys to integers\n        id2label = {int(k): v for k, v in id2label.items()}\n        \n        # Create label list sorted by ID\n        label_list = [id2label[i] for i in range(len(id2label))]\n        \n        # Update Config with model parameters\n        for key, value in model_config.items():\n            setattr(Config, key, value)\n        \n        print(f\"Loaded configuration for model: {Config.bert_model_name}\")\n        print(f\"Number of labels: {len(label_list)}\")\n        \n        # 2. Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n        \n        # 3. Preprocess test dataset\n        print(\"\\nPreprocessing test dataset...\")\n        test_hier = preprocess_single_dataset(test_ds, label2id)\n        print(f\"Test examples after preprocessing: {len(test_hier)}\")\n        \n        # 4. Tokenize test dataset\n        print(\"Tokenizing test dataset...\")\n        test_tokenized = tokenize_single_dataset(test_hier, tokenizer, label2id)\n        \n        # 5. Create data loader\n        test_loader = create_single_loader(test_tokenized)\n        print(f\"Test batches: {len(test_loader)}\")\n        \n        # 6. Initialize model\n        print(\"\\nInitializing model...\")\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            model_config=model_config,\n            class_weights=None\n        ).to(device)\n        \n        # 7. Load model weights\n        model.load_state_dict(torch.load(model_path, map_location=device))\n        print(\"Model weights loaded successfully\")\n        \n        # 8. Evaluate on test set\n        print(\"\\nEvaluating on test set...\")\n        test_metrics = evaluate_metrics(model, test_loader, device, label_list, Config.output_dir)\n        \n        # 9. Save test metrics\n        metrics_path = os.path.join(Config.output_dir, \"test_metrics.json\")\n        with open(metrics_path, 'w') as f:\n            json.dump(test_metrics, f, indent=2)\n            \n        print(f\"\\n{'='*30} TEST RESULTS {'='*30}\")\n        print(f\"Weighted F1: {test_metrics['weighted_f1']:.4f}\")\n        print(f\"Macro F1:    {test_metrics['macro_f1']:.4f}\")\n        print(f\"Accuracy:    {test_metrics['accuracy']:.4f}\")\n        print(f\"Macro Precision: {test_metrics['macro_precision']:.4f}\")\n        print(f\"Macro Recall:    {test_metrics['macro_recall']:.4f}\")\n        print(f\"Latency:     {test_metrics['latency_ms_per_doc']:.2f} ms/doc\")\n        print(f\"Evaluation time: {test_metrics['eval_time_seconds']:.2f} seconds\")\n        print(f\"Metrics saved to: {metrics_path}\")\n        \n        print(\"\\nPer-class Metrics:\")\n        for label in label_list:\n            print(f\"  {label}:\")\n            print(f\"    F1:       {test_metrics['per_label_f1'][label]:.4f}\")\n            print(f\"    Precision: {test_metrics['per_label_precision'][label]:.4f}\")\n            print(f\"    Recall:    {test_metrics['per_label_recall'][label]:.4f}\")\n            \n        total_time = time.time() - start_time\n        print(f\"\\nEvaluation completed in {total_time:.2f} seconds\")\n        print(f\"{'='*30} EVALUATION COMPLETE {'='*30}\")\n        \n        return test_metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"EVALUATION FAILED!\")\n        print(f\"Error: {str(e)}\")\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Evaluation error at {datetime.now()}\\n\")\n            f.write(str(e))\n        return None\n\nif __name__ == \"__main__\":\n    evaluate_test_set()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
