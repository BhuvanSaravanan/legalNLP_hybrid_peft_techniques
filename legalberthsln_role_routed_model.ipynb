{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom huggingface_hub import notebook_login, create_repo, upload_folder\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport psutil\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nimport torch.nn as nn\nfrom torchcrf import CRF\nimport random\nfrom collections import Counter\nfrom datasets import concatenate_datasets\nimport nltk\nfrom nltk.corpus import wordnet\nfrom transformers import get_linear_schedule_with_warmup\n\n# Download required NLTK data\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('punkt', quiet=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    # Optimized hyperparameters\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    context_hidden_size = 256\n    max_num_sentences = 48\n    max_length = 96\n    dropout_rate = 0.4\n    gamma = 2.0\n    weight_decay = 1e-5\n\n    # Training parameters\n    epochs = 8\n    batch_size = 4\n    learning_rate = 5e-5\n    warmup_ratio = 0.1\n    patience = 3\n    gradient_accumulation_steps = 2\n\n    # Role-Routed Adapter parameters\n    adapter_intermediate_size = 256\n    num_roles = 13  # 13 rhetorical roles (e.g., FAC, ARG_RESPONDENT, etc.)\n\n    # Paths and repo info\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-role-routed\"\n    output_dir = \"./role_routed_model\"\n    save_checkpoint = \"best_model\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to Hugging Face Hub\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import notebook_login","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\nsplits = {\n    'train': 'data/train-00000-of-00001-bb0092e0d8549337.parquet',\n    'dev': 'data/dev-00000-of-00001-af55705c75623915.parquet',\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntrain_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"train\"])\ndev_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"dev\"])\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\n\n# Convert to Hugging Face Datasets\ntrain_ds = Dataset.from_pandas(train_df)\ndev_ds = Dataset.from_pandas(dev_df)\ntest_ds = Dataset.from_pandas(test_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_synonyms(word):\n    \"\"\"Get synonyms for data augmentation\"\"\"\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonym = lemma.name().replace(\"_\", \" \").lower()\n            if synonym != word and len(synonym) > 1:\n                synonyms.add(synonym)\n    return list(synonyms) if synonyms else [word]\n\ndef augment_sentence(sent):\n    \"\"\"Enhanced data augmentation with synonym replacement\"\"\"\n    if len(sent.strip()) == 0:\n        return sent\n\n    words = nltk.word_tokenize(sent)\n    if len(words) < 2:\n        return sent\n\n    choice = random.choices([1, 2, 3, 4], weights=[0.4, 0.2, 0.3, 0.1])[0]\n\n    if choice == 1:  # Synonym replacement\n        idx = random.randint(0, len(words)-1)\n        synonyms = get_synonyms(words[idx])\n        if synonyms and len(synonyms) > 0:\n            words[idx] = random.choice(synonyms)\n\n    elif choice == 2 and len(words) >= 4:  # Random deletion\n        del_idx = random.randint(0, len(words)-1)\n        del words[del_idx]\n\n    elif choice == 3 and len(words) >= 3:  # Word swap\n        i, j = random.sample(range(len(words)), 2)\n        words[i], words[j] = words[j], words[i]\n\n    elif choice == 4:  # Random insertion\n        idx = random.randint(0, len(words)-1)\n        synonyms = get_synonyms(words[idx])\n        if synonyms and len(synonyms) > 0:\n            words.insert(idx, random.choice(synonyms))\n\n    return \" \".join(words)\n\ndef prepare_hierarchical_datasets(train_ds, dev_ds, test_ds):\n    \"\"\"Optimized dataset preparation with efficient sampling\"\"\"\n    print(\"Preprocessing datasets with efficient sampling...\")\n\n    def get_spans_and_labels(example):\n        spans = []\n        labels = []\n        if example.get('annotations') and len(example['annotations']) > 0:\n            if example['annotations'][0].get('result'):\n                for ann in example['annotations'][0]['result']:\n                    if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                        spans.append(ann['value']['text'])\n                        labels.append(ann['value']['labels'][0])\n        return {'spans': spans, 'labels': labels}\n\n    train_ds = train_ds.map(get_spans_and_labels)\n    dev_ds = dev_ds.map(get_spans_and_labels)\n    test_ds = test_ds.map(get_spans_and_labels)\n\n    train_ds = train_ds.filter(lambda x: len(x['spans']) > 0)\n    dev_ds = dev_ds.filter(lambda x: len(x['spans']) > 0)\n    test_ds = test_ds.filter(lambda x: len(x['spans']) > 0)\n\n    def prepare_for_hierarchical(example):\n        return {'text': example['spans'], 'label': example['labels']}\n\n    train_hier = train_ds.map(prepare_for_hierarchical)\n    dev_hier = dev_ds.map(prepare_for_hierarchical)\n    test_hier = test_ds.map(prepare_for_hierarchical)\n\n    all_labels = set()\n    for example in train_hier:\n        all_labels.update(example['label'])\n    label_list = sorted(list(all_labels))\n    label2id = {l: i for i, l in enumerate(label_list)}\n    id2label = {i: l for i, l in enumerate(label_list)}\n\n    print(f\"Identified {len(label_list)} labels: {label_list}\")\n\n    def augment_dataset(dataset, label2id):\n        label_counts = Counter()\n        for example in dataset:\n            label_counts.update(example['label'])\n\n        rare_classes = [label for label, count in label_counts.items() if count < 10]\n        print(f\"Rare classes (<10 samples): {rare_classes}\")\n\n        augmented_examples = []\n        for example in dataset:\n            labels = example['label']\n            copies = 1\n            if any(label in rare_classes for label in labels):\n                copies = 3\n            for _ in range(copies):\n                augmented_text = [\n                    augment_sentence(sent) if random.random() < 0.5 and sent.strip() else sent\n                    for sent in example['text']\n                ]\n                augmented_examples.append({\n                    'text': augmented_text,\n                    'label': labels.copy()\n                })\n\n        print(f\"Added {len(augmented_examples)} augmented examples\")\n        return concatenate_datasets([dataset, Dataset.from_list(augmented_examples)])\n\n    train_hier = augment_dataset(train_hier, label2id)\n\n    label_counts = Counter()\n    for example in train_hier:\n        label_counts.update(example['label'])\n\n    max_count = min(1000, max(label_counts.values()))\n    balanced_examples = []\n    for label in label_list:\n        class_examples = [ex for ex in train_hier if label in ex['label']]\n        current_count = label_counts[label]\n        needed = max(0, max_count - current_count)\n        if needed > 0 and class_examples:\n            balanced_examples.extend(class_examples)\n            duplicates = min(needed, len(class_examples))\n            balanced_examples.extend(random.choices(class_examples, k=duplicates))\n        else:\n            balanced_examples.extend(class_examples)\n\n    train_hier = Dataset.from_list(balanced_examples)\n    print(f\"Created balanced dataset with {len(train_hier)} examples\")\n\n    return train_hier, dev_hier, test_hier, label2id, id2label, label_list\n\nclass RoleRoutedAdapter(nn.Module):\n    \"\"\"Role-Routed Adapter with 13 parallel adapters for rhetorical roles\"\"\"\n    def __init__(self, config, role_count, intermediate_size=64):\n        super().__init__()\n        self.config = config\n        self.role_count = role_count\n        self.intermediate_size = intermediate_size\n\n        # 13 parallel adapters for each role\n        self.adapters = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(config.hidden_size, intermediate_size),\n                nn.GELU(),\n                nn.Linear(intermediate_size, config.hidden_size)\n            ) for _ in range(role_count)\n        ])\n\n        # Router to coordinate adapters\n        self.router = nn.Sequential(\n            nn.Linear(config.hidden_size, 256),\n            nn.Tanh(),\n            nn.Linear(256, role_count)\n        )\n\n        for adapter in self.adapters:\n            for layer in adapter:\n                if isinstance(layer, nn.Linear):\n                    layer.weight.data.normal_(mean=0.0, std=0.02)\n                    if layer.bias is not None:\n                        layer.bias.data.zero_()\n\n        for layer in self.router:\n            if isinstance(layer, nn.Linear):\n                layer.weight.data.normal_(mean=0.0, std=0.02)\n                if layer.bias is not None:\n                    layer.bias.data.zero_()\n\n    def forward(self, x):\n        # x shape: (batch_size, num_sentences, hidden_size)\n        batch_size, seq_len, hidden_size = x.shape\n\n        # Use average embedding for routing (since this is sentence-level)\n        avg_emb = x.mean(dim=1)  # (batch_size, hidden_size)\n        router_logits = self.router(avg_emb)  # (batch_size, role_count)\n        routing_weights = torch.softmax(router_logits, dim=-1)  # (batch_size, role_count)\n\n        # Compute outputs for all 13 adapters\n        adapter_outputs = [adapter(x) for adapter in self.adapters]  # List of (batch_size, num_sentences, hidden_size)\n        adapter_outputs = torch.stack(adapter_outputs, dim=1)  # (batch_size, role_count, num_sentences, hidden_size)\n\n        # Apply routing weights\n        weighted_output = torch.einsum('br,brsh->bsh', routing_weights, adapter_outputs)\n\n        return x + weighted_output\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings with dropout\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.dropout = nn.Dropout(Config.dropout_rate)\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return self.dropout(x + self.position_emb(positions))\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Custom transformer layer with 13 parallel adapters per the image schematic\"\"\"\n    def __init__(self, d_model, nhead=8, dim_feedforward=1024, dropout=0.2, num_roles=Config.num_roles):\n        super().__init__()\n        self.d_model = d_model\n\n        # Multi-Head Attention\n        self.multihead_attn = nn.MultiheadAttention(\n            embed_dim=d_model,\n            num_heads=nhead,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        # First set of 13 parallel adapters after attention\n        self.adapter1 = RoleRoutedAdapter(\n            config=AutoConfig.from_pretrained(Config.bert_model_name),\n            role_count=num_roles,\n            intermediate_size=Config.adapter_intermediate_size\n        )\n\n        # Feed Forward\n        self.feed_forward = nn.Sequential(\n            nn.Linear(d_model, dim_feedforward),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim_feedforward, d_model)\n        )\n        self.norm2 = nn.LayerNorm(d_model)\n        # Second set of 13 parallel adapters after feed forward\n        self.adapter2 = RoleRoutedAdapter(\n            config=AutoConfig.from_pretrained(Config.bert_model_name),\n            role_count=num_roles,\n            intermediate_size=Config.adapter_intermediate_size\n        )\n\n    def forward(self, x):\n        # Multi-Head Attention with residual connection\n        attn_output, _ = self.multihead_attn(x, x, x)\n        x = self.norm1(x + attn_output)\n        # First set of 13 parallel adapters\n        x = self.adapter1(x)\n\n        # Feed Forward with residual connection\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + ff_output)\n        # Second set of 13 parallel adapters\n        x = self.adapter2(x)\n\n        return x\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with residual connection\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.3):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, input_size*2)\n        self.linear2 = nn.Linear(input_size*2, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n        self.layer_norm = nn.LayerNorm(input_size*2)\n        self.residual_proj = nn.Linear(input_size, num_labels)\n\n    def forward(self, x):\n        residual = x\n        x = self.linear1(x)\n        x = self.layer_norm(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        return self.linear2(x) + self.residual_proj(residual)\n\nclass FocalCRF(nn.Module):\n    \"\"\"Fixed CRF with focal loss\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]\n            valid_counts = mask.sum(dim=1)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"HSLN model with Role-Routed Adapters in transformer layer\"\"\"\n    def __init__(self, num_labels, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Load base Legal-BERT without QLoRA\n        self.bert = AutoModel.from_pretrained(\n            Config.bert_model_name,\n            device_map=\"auto\"\n        )\n\n        # Feature extraction layers\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.sent_projection = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n\n        # Context encoding with custom transformer layer\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(d_model=self.bert.config.hidden_size)\n\n        # Emission and CRF\n        self.emission = EmissionLayer(input_size=self.bert.config.hidden_size, num_labels=num_labels)\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(input_ids=flat_input_ids, attention_mask=flat_mask).last_hidden_state\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_projection(sent_emb)\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0\n\n        if labels is not None:\n            loss = self.crf(emissions, labels, mask=mask, class_weights=self.class_weights)\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\ndef tokenize_datasets(train_hier, dev_hier, test_hier, label2id):\n    \"\"\"Tokenize datasets\"\"\"\n    print(\"Tokenizing datasets...\")\n    tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n    def tokenize_document(example):\n        sentences = example['text'][:Config.max_num_sentences]\n        labels = example['label'][:Config.max_num_sentences]\n\n        tokenized = tokenizer(\n            sentences,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=Config.max_length,\n            return_tensors=\"pt\",\n            return_attention_mask=True\n        )\n\n        pad_len = Config.max_num_sentences - len(sentences)\n        if pad_len > 0:\n            pad_shape = (pad_len, Config.max_length)\n            tokenized[\"input_ids\"] = torch.cat([\n                tokenized[\"input_ids\"],\n                torch.full(pad_shape, tokenizer.pad_token_id, dtype=torch.long)\n            ])\n            tokenized[\"attention_mask\"] = torch.cat([\n                tokenized[\"attention_mask\"],\n                torch.zeros(pad_shape, dtype=torch.long)\n            ])\n            labels += [list(label2id.keys())[0]] * pad_len\n\n        label_ids = torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n        return {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"], \"labels\": label_ids}\n\n    train_tokenized = train_hier.map(tokenize_document, batched=False)\n    dev_tokenized = dev_hier.map(tokenize_document, batched=False)\n    test_tokenized = test_hier.map(tokenize_document, batched=False)\n    return train_tokenized, dev_tokenized, test_tokenized, tokenizer\n\nclass HierarchicalDataset(Dataset):\n    \"\"\"Dataset class\"\"\"\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long) if not isinstance(item[\"input_ids\"], torch.Tensor) else item[\"input_ids\"],\n            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long) if not isinstance(item[\"attention_mask\"], torch.Tensor) else item[\"attention_mask\"],\n            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long) if not isinstance(item[\"labels\"], torch.Tensor) else item[\"labels\"]\n        }\n\ndef collate_fn(batch):\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\ndef create_data_loaders(train_tokenized, dev_tokenized, test_tokenized):\n    train_loader = DataLoader(HierarchicalDataset(train_tokenized), batch_size=Config.batch_size, shuffle=True, collate_fn=collate_fn, pin_memory=True)\n    dev_loader = DataLoader(HierarchicalDataset(dev_tokenized), batch_size=Config.batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n    test_loader = DataLoader(HierarchicalDataset(test_tokenized), batch_size=Config.batch_size, shuffle=False, collate_fn=collate_fn, pin_memory=True)\n    return train_loader, dev_loader, test_loader\n\ndef compute_class_weights(train_hier, label2id):\n    label_counts = {label: 0 for label in label2id}\n    for example in train_hier:\n        for label in example['label']:\n            label_counts[label] += 1\n    total_samples = sum(label_counts.values())\n    weights = [(total_samples / (label_counts[label] + 1e-5)) ** 0.5 for label in label2id]\n    return torch.tensor(weights, dtype=torch.float32) / min(weights)\n\ndef train_model(model, train_loader, dev_loader, optimizer, device, epochs, label_list):\n    \"\"\"Training loop\"\"\"\n    print(f\"\\n{'='*30} TRAINING STARTED {'='*30}\")\n    total_steps = len(train_loader) * epochs // Config.gradient_accumulation_steps\n    warmup_steps = int(total_steps * Config.warmup_ratio)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n\n    model.train()\n    best_dev_f1 = 0\n    best_macro_f1 = 0\n    no_improve = 0\n    training_start = time.time()\n    history = []\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        optimizer.zero_grad()\n\n        for batch_idx, batch in enumerate(train_loader):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs[\"loss\"] / Config.gradient_accumulation_steps\n            loss.backward()\n\n            total_loss += loss.item()\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=attention_mask[:, :, 0] > 0)\n\n            flat_labels = labels.cpu().numpy().flatten()\n            flat_preds = []\n            for i, seq in enumerate(preds):\n                seq_len = (attention_mask[i, :, 0] > 0).sum().item()\n                flat_preds.extend(seq[:seq_len])\n            flat_preds += [0] * (len(flat_labels) - len(flat_preds))\n            flat_preds = np.array(flat_preds)\n\n            all_preds.extend(flat_preds)\n            all_labels.extend(flat_labels)\n\n            if (batch_idx + 1) % Config.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item() * Config.gradient_accumulation_steps:.4f}\")\n\n        if (batch_idx + 1) % Config.gradient_accumulation_steps != 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        epoch_time = time.time() - epoch_start\n        train_f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n\n        model.eval()\n        dev_metrics = evaluate_metrics(model, dev_loader, device, label_list)\n        dev_f1 = dev_metrics[\"weighted_f1\"]\n        dev_macro = dev_metrics[\"macro_f1\"]\n\n        history.append({'epoch': epoch+1, 'train_loss': total_loss/len(train_loader), 'train_f1': train_f1, 'dev_f1': dev_f1, 'dev_macro_f1': dev_macro})\n\n        print(f\"Epoch {epoch+1} completed in {epoch_time:.2f}s | Train F1: {train_f1:.4f} | Dev F1: {dev_f1:.4f} | Macro F1: {dev_macro:.4f}\")\n\n        if dev_f1 > best_dev_f1 or dev_macro > best_macro_f1:\n            if dev_f1 > best_dev_f1: best_dev_f1 = dev_f1\n            if dev_macro > best_macro_f1: best_macro_f1 = dev_macro\n            no_improve = 0\n            torch.save(model.state_dict(), os.path.join(Config.output_dir, \"best_model.pt\"))\n            print(f\"New best model saved with F1: {dev_f1:.4f}, Macro F1: {dev_macro:.4f}\")\n        else:\n            no_improve += 1\n            if no_improve >= Config.patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    training_time = time.time() - training_start\n    print(f\"Training completed in {training_time:.2f} seconds\")\n    model.load_state_dict(torch.load(os.path.join(Config.output_dir, \"best_model.pt\")))\n    return model, history\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Evaluation\"\"\"\n    model.eval()\n    all_preds, all_labels = [], []\n    total_time = 0\n    n_docs = 0\n    n_sentences = 0\n    eval_start = time.time()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            mask = attention_mask[:, :, 0] > 0\n\n            start = time.time()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            end = time.time()\n\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=mask)\n\n            for i in range(len(labels)):\n                seq_len = mask[i].sum().item()\n                if seq_len > 0:\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n            total_time += (end - start)\n            n_docs += input_ids.shape[0]\n            n_sentences += mask.sum().item()\n\n    eval_time = time.time() - eval_start\n    report = classification_report(all_labels, all_preds, labels=list(range(len(label_list))), target_names=label_list, output_dict=True, zero_division=0)\n    return {\n        \"macro_f1\": report['macro avg']['f1-score'],\n        \"weighted_f1\": report['weighted avg']['f1-score'],\n        \"accuracy\": report['accuracy'],\n        \"per_label_f1\": {label: report[label]['f1-score'] for label in label_list},\n        \"latency_ms_per_doc\": (total_time / n_docs) * 1000 if n_docs else 0,\n        \"latency_ms_per_sentence\": (total_time / n_sentences) * 1000 if n_sentences else 0,\n        \"eval_time_seconds\": eval_time,\n        \"num_samples\": n_docs\n    }\n\ndef get_model_size_mb(model):\n    param_size = sum(param.nelement() * param.element_size() for param in model.parameters())\n    buffer_size = sum(buffer.nelement() * buffer.element_size() for buffer in model.buffers())\n    return (param_size + buffer_size) / (1024 ** 2)\n\ndef get_memory_footprint():\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / (1024 ** 3)\n\ndef save_checkpoint(model, tokenizer, metrics, label2id, config, save_dir, checkpoint_name):\n    \"\"\"Save model checkpoint\"\"\"\n    checkpoint_path = os.path.join(save_dir, checkpoint_name)\n    os.makedirs(checkpoint_path, exist_ok=True)\n    torch.save(model.state_dict(), os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n    tokenizer.save_pretrained(checkpoint_path)\n\n    with open(os.path.join(checkpoint_path, \"config.json\"), \"w\") as f:\n        json.dump({\n            \"label2id\": label2id,\n            \"id2label\": {i: l for l, i in label2id.items()},\n            \"model_config\": {\n                \"bert_model_name\": config.bert_model_name,\n                \"max_num_sentences\": config.max_num_sentences,\n                \"max_length\": config.max_length,\n                \"dropout_rate\": config.dropout_rate,\n                \"gamma\": config.gamma,\n                \"num_roles\": config.num_roles,\n                \"adapter_intermediate_size\": config.adapter_intermediate_size\n            }\n        }, f, indent=2)\n\n    with open(os.path.join(checkpoint_path, \"metrics.json\"), \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    print(f\"Checkpoint saved to {checkpoint_path}\")\n    return checkpoint_path\n\ndef upload_to_huggingface(save_path, repo_id):\n    \"\"\"Upload to Hugging Face\"\"\"\n    create_repo(repo_id, exist_ok=True, token=True)\n    upload_folder(repo_id=repo_id, folder_path=save_path, commit_message=\"Hierarchical Legal Model with Role-Routed Adapters\", repo_type=\"model\", token=True)\n    print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n\ndef main():\n    \"\"\"Main pipeline\"\"\"\n    try:\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\\nHIERARCHICAL LEGAL MODEL WITH ROLE-ROUTED ADAPTERS\\nTimestamp: {datetime.now().isoformat()}\\nDevice: {device}\\n{'='*50}\\n\")\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        train_hier, dev_hier, test_hier, label2id, id2label, label_list = prepare_hierarchical_datasets(train_ds, dev_ds, test_ds)\n        class_weights = compute_class_weights(train_hier, label2id).to(device)\n        train_tokenized, dev_tokenized, test_tokenized, tokenizer = tokenize_datasets(train_hier, dev_hier, test_hier, label2id)\n        train_loader, dev_loader, test_loader = create_data_loaders(train_tokenized, dev_tokenized, test_tokenized)\n\n        model = ImprovedHSLNModel(num_labels=len(label2id), class_weights=class_weights).to(device)\n        optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n\n        model, history = train_model(model, train_loader, dev_loader, optimizer, device, Config.epochs, label_list)\n        dev_metrics = evaluate_metrics(model, dev_loader, device, label_list)\n        train_metrics = evaluate_metrics(model, train_loader, device, label_list)\n\n        metrics = {\n            \"train\": train_metrics,\n            \"dev\": dev_metrics,\n            \"model_size_mb\": get_model_size_mb(model),\n            \"training_memory_footprint_gb\": get_memory_footprint(),\n            \"label2id\": label2id,\n            \"id2label\": id2label,\n            \"training_time\": time.time() - start_time,\n            \"training_history\": history\n        }\n\n        checkpoint_path = save_checkpoint(model, tokenizer, metrics, label2id, Config, Config.output_dir, Config.save_checkpoint)\n        upload_to_huggingface(checkpoint_path, Config.hf_repo_id)\n\n        print(\"\\n==== FINAL METRICS ====\")\n        print(f\"Train Weighted F1: {train_metrics['weighted_f1']:.4f} | Macro F1: {train_metrics['macro_f1']:.4f} | Accuracy: {train_metrics['accuracy']:.4f}\")\n        print(f\"Dev Weighted F1: {dev_metrics['weighted_f1']:.4f} | Macro F1: {dev_metrics['macro_f1']:.4f} | Accuracy: {dev_metrics['accuracy']:.4f}\")\n        print(f\"Model Size: {metrics['model_size_mb']:.2f} MB | Training Time: {metrics['training_time']:.2f} seconds\")\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\\nPIPELINE FAILED!\\nError: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Pipeline error at {datetime.now()}\\n{str(e)}\\n{traceback.format_exc()}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
