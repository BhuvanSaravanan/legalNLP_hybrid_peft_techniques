{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\nfrom huggingface_hub import notebook_login, create_repo, upload_folder\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport psutil\nimport numpy as np\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nimport random\nfrom collections import Counter\nfrom datasets import concatenate_datasets\nfrom peft import LoraConfig, get_peft_model\n# Change 1: Import AdaLoraConfig\nfrom peft import AdaLoraConfig  # AdaLoRA instead of LoRA\nimport nltk\nfrom nltk.corpus import wordnet\nfrom transformers import get_linear_schedule_with_warmup","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download required NLTK data\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\n\nnltk.download('punkt_tab')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Config:\n    # Optimized hyperparameters\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    context_hidden_size = 256\n    max_num_sentences = 48\n    max_length = 128\n    dropout_rate = 0.4\n    gamma = 2.0\n    weight_decay = 1e-5\n\n    # Training parameters\n    epochs = 8\n    batch_size = 4\n    learning_rate = 5e-5\n    warmup_ratio = 0.1\n    patience = 3\n    gradient_accumulation_steps = 2\n\n    # Change 2: Update to AdaLoRA parameters\n    # AdaLoRA parameters\n    adalora_init_r = 128\n    adalora_target_r = 4608  # Total rank budget (36 modules * 128)\n    adalora_beta1 = 0.85\n    adalora_beta2 = 0.85\n    adalora_tinit = 200\n    adalora_tfinal = 1000\n    adalora_deltaT = 10\n    lora_alpha = 64\n    lora_dropout = 0.15\n    lora_target_modules = [\"query\", \"value\", \"key\"]\n\n    # Paths and repo info\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-adalora-final\"\n    output_dir = \"./final_hierarchical_model\"\n    save_checkpoint = \"best_model\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to Hugging Face Hub\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import notebook_login","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets\nsplits = {\n    'train': 'data/train-00000-of-00001-bb0092e0d8549337.parquet',\n    'dev': 'data/dev-00000-of-00001-af55705c75623915.parquet',\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntrain_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"train\"])\ndev_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"dev\"])\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\n\n# Convert to Hugging Face Datasets\ntrain_ds = Dataset.from_pandas(train_df)\ndev_ds = Dataset.from_pandas(dev_df)\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_synonyms(word):\n    \"\"\"Get synonyms for data augmentation\"\"\"\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonym = lemma.name().replace(\"_\", \" \").lower()\n            if synonym != word and len(synonym) > 1:\n                synonyms.add(synonym)\n    return list(synonyms) if synonyms else [word]\n\ndef augment_sentence(sent):\n    \"\"\"Enhanced data augmentation with synonym replacement\"\"\"\n    if len(sent.strip()) == 0:\n        return sent\n\n    words = nltk.word_tokenize(sent)\n    if len(words) < 2:\n        return sent\n\n    # Choose an augmentation technique\n    choice = random.choices([1, 2, 3, 4], weights=[0.4, 0.2, 0.3, 0.1])[0]\n\n    if choice == 1:  # Synonym replacement\n        idx = random.randint(0, len(words)-1)\n        synonyms = get_synonyms(words[idx])\n        if synonyms and len(synonyms) > 0:\n            words[idx] = random.choice(synonyms)\n\n    elif choice == 2 and len(words) >= 4:  # Random deletion\n        del_idx = random.randint(0, len(words)-1)\n        del words[del_idx]\n\n    elif choice == 3 and len(words) >= 3:  # Word swap\n        i, j = random.sample(range(len(words)), 2)\n        words[i], words[j] = words[j], words[i]\n\n    elif choice == 4:  # Random insertion\n        idx = random.randint(0, len(words)-1)\n        synonyms = get_synonyms(words[idx])\n        if synonyms and len(synonyms) > 0:\n            words.insert(idx, random.choice(synonyms))\n\n    return \" \".join(words)\n\ndef prepare_hierarchical_datasets(train_ds, dev_ds, test_ds):\n    \"\"\"Optimized dataset preparation with efficient sampling\"\"\"\n    print(\"Preprocessing datasets with efficient sampling...\")\n\n    def get_spans_and_labels(example):\n        spans = []\n        labels = []\n        if example.get('annotations') and len(example['annotations']) > 0:\n            if example['annotations'][0].get('result'):\n                for ann in example['annotations'][0]['result']:\n                    if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                        spans.append(ann['value']['text'])\n                        labels.append(ann['value']['labels'][0])\n        return {'spans': spans, 'labels': labels}\n\n    # Apply to all splits\n    train_ds = train_ds.map(get_spans_and_labels)\n    dev_ds = dev_ds.map(get_spans_and_labels)\n    test_ds = test_ds.map(get_spans_and_labels)\n\n    # Filter out empty examples\n    train_ds = train_ds.filter(lambda x: len(x['spans']) > 0)\n    dev_ds = dev_ds.filter(lambda x: len(x['spans']) > 0)\n    test_ds = test_ds.filter(lambda x: len(x['spans']) > 0)\n\n    def prepare_for_hierarchical(example):\n        return {'text': example['spans'], 'label': example['labels']}\n\n    train_hier = train_ds.map(prepare_for_hierarchical)\n    dev_hier = dev_ds.map(prepare_for_hierarchical)\n    test_hier = test_ds.map(prepare_for_hierarchical)\n\n    # Build label mapping\n    all_labels = set()\n    for example in train_hier:\n        all_labels.update(example['label'])\n    label_list = sorted(list(all_labels))\n    label2id = {l: i for i, l in enumerate(label_list)}\n    id2label = {i: l for i, l in enumerate(label_list)}\n\n    print(f\"Identified {len(label_list)} labels: {label_list}\")\n\n    # 1. Efficient data augmentation (only for rare classes)\n    def augment_dataset(dataset, label2id):\n        label_counts = Counter()\n        for example in dataset:\n            label_counts.update(example['label'])\n\n        # Identify rare classes\n        rare_classes = [label for label, count in label_counts.items() if count < 10]\n        print(f\"Rare classes (<10 samples): {rare_classes}\")\n\n        augmented_examples = []\n        for example in dataset:\n            labels = example['label']\n            copies = 1\n\n            if any(label in rare_classes for label in labels):\n                copies = 3  # Moderate augmentation for rare classes\n\n            # Create augmented copies\n            for _ in range(copies):\n                augmented_text = [\n                    augment_sentence(sent) if random.random() < 0.5 and sent.strip() else sent\n                    for sent in example['text']\n                ]\n                augmented_examples.append({\n                    'text': augmented_text,\n                    'label': labels.copy()\n                })\n\n        print(f\"Added {len(augmented_examples)} augmented examples\")\n        return concatenate_datasets([dataset, Dataset.from_list(augmented_examples)])\n\n    # Apply augmentation to training set\n    train_hier = augment_dataset(train_hier, label2id)\n\n    # 2. Efficient class balancing\n    label_counts = Counter()\n    for example in train_hier:\n        label_counts.update(example['label'])\n\n    # Calculate target counts - cap at 1000 samples per class\n    max_count = min(1000, max(label_counts.values()))\n    class_weights = {}\n\n    # Create balanced dataset\n    balanced_examples = []\n    for label in label_list:\n        # Collect examples containing this label\n        class_examples = [ex for ex in train_hier if label in ex['label']]\n        current_count = label_counts[label]\n\n        # Calculate how many to add\n        needed = max(0, max_count - current_count)\n\n        # If we need to add examples, duplicate existing ones\n        if needed > 0 and class_examples:\n            # Add existing examples\n            balanced_examples.extend(class_examples)\n\n            # Add duplicated examples\n            duplicates = min(needed, len(class_examples))\n            balanced_examples.extend(random.choices(class_examples, k=duplicates))\n        else:\n            balanced_examples.extend(class_examples)\n\n    # Create balanced dataset\n    train_hier = Dataset.from_list(balanced_examples)\n    print(f\"Created balanced dataset with {len(train_hier)} examples\")\n\n    return train_hier, dev_hier, test_hier, label2id, id2label, label_list\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings with dropout\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.dropout = nn.Dropout(Config.dropout_rate)\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return self.dropout(x + self.position_emb(positions))\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Enhanced transformer context modeling\"\"\"\n    def __init__(self, d_model, nhead=8, dim_feedforward=1024, dropout=0.2):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n            activation='gelu'\n        )\n        self.transformer_encoder = nn.TransformerEncoder(\n            self.encoder_layer,\n            num_layers=2\n        )\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Enhanced emission layer with residual connection\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.3):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, input_size*2)\n        self.linear2 = nn.Linear(input_size*2, num_labels)\n        self.dropout = nn.Dropout(dropout)\n        self.gelu = nn.GELU()\n        self.layer_norm = nn.LayerNorm(input_size*2)\n        self.residual_proj = nn.Linear(input_size, num_labels)\n\n    def forward(self, x):\n        residual = x\n        x = self.linear1(x)\n        x = self.layer_norm(x)\n        x = self.gelu(x)\n        x = self.dropout(x)\n        return self.linear2(x) + self.residual_proj(residual)\n\nclass FocalCRF(nn.Module):\n    \"\"\"Fixed CRF with focal loss - no label smoothing\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        # Compute standard CRF loss\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n\n        # Apply focal loss transformation\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        # Apply class weights if provided\n        if class_weights is not None:\n            # Compute weight per sequence (average of tag weights)\n            weights_per_tag = class_weights[tags]  # (batch_size, seq_len)\n            valid_counts = mask.sum(dim=1)  # (batch_size,)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"Fixed hierarchical model with CRF fix\"\"\"\n    def __init__(self, num_labels, total_steps, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Sentence encoding with AdaLoRA\n        base_bert = AutoModel.from_pretrained(Config.bert_model_name)\n\n        # Configure AdaLoRA instead of LoRA\n        adalora_config = AdaLoraConfig(\n            init_r=Config.adalora_init_r,\n            target_r=Config.adalora_target_r,\n            beta1=Config.adalora_beta1,\n            beta2=Config.adalora_beta2,\n            tinit=Config.adalora_tinit,\n            tfinal=Config.adalora_tfinal,\n            deltaT=Config.adalora_deltaT,\n            lora_alpha=Config.lora_alpha,\n            lora_dropout=Config.lora_dropout,\n            target_modules=Config.lora_target_modules,\n            bias=\"none\",\n            total_step=total_steps  # Add total_step parameter\n        )\n        self.bert = get_peft_model(base_bert, adalora_config)\n        self.bert.print_trainable_parameters()\n\n        # Enhanced feature extraction\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.sent_projection = nn.Linear(\n            self.bert.config.hidden_size,\n            self.bert.config.hidden_size\n        )\n\n        # Context encoding\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n\n        # Emission layer\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n\n        # CRF layer with focal loss\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_projection(sent_emb)\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask,\n                class_weights=self.class_weights\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\ndef tokenize_datasets(train_hier, dev_hier, test_hier, label2id):\n    \"\"\"Tokenize datasets with tensor conversion\"\"\"\n    print(\"Tokenizing datasets...\")\n    tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n\n        # Truncate to max sentences\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n\n        # Tokenize sentences\n        tokenized = tokenizer(\n            sentences,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=Config.max_length,\n            return_tensors=\"pt\",\n            return_attention_mask=True\n        )\n\n        # Pad to max sentences\n        pad_len = Config.max_num_sentences - len(sentences)\n        if pad_len > 0:\n            pad_shape = (pad_len, Config.max_length)\n            tokenized[\"input_ids\"] = torch.cat([\n                tokenized[\"input_ids\"],\n                torch.full(pad_shape, tokenizer.pad_token_id, dtype=torch.long)\n            ])\n            tokenized[\"attention_mask\"] = torch.cat([\n                tokenized[\"attention_mask\"],\n                torch.zeros(pad_shape, dtype=torch.long)\n            ])\n            labels += [list(label2id.keys())[0]] * pad_len\n\n        label_ids = torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n\n        return {\n            \"input_ids\": tokenized[\"input_ids\"],\n            \"attention_mask\": tokenized[\"attention_mask\"],\n            \"labels\": label_ids\n        }\n\n    # Tokenize datasets with batched processing disabled\n    train_tokenized = train_hier.map(tokenize_document, batched=False)\n    dev_tokenized = dev_hier.map(tokenize_document, batched=False)\n    test_tokenized = test_hier.map(tokenize_document, batched=False)\n\n    return train_tokenized, dev_tokenized, test_tokenized, tokenizer\n\nclass HierarchicalDataset(Dataset):\n    \"\"\"Dataset class with robust tensor conversion\"\"\"\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n\n        # Convert all elements to tensors\n        input_ids = item[\"input_ids\"]\n        attention_mask = item[\"attention_mask\"]\n        labels = item[\"labels\"]\n\n        # Convert to tensors if needed\n        if not isinstance(input_ids, torch.Tensor):\n            input_ids = torch.tensor(input_ids, dtype=torch.long)\n        if not isinstance(attention_mask, torch.Tensor):\n            attention_mask = torch.tensor(attention_mask, dtype=torch.long)\n        if not isinstance(labels, torch.Tensor):\n            labels = torch.tensor(labels, dtype=torch.long)\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }\n\ndef collate_fn(batch):\n    # Convert all items to tensors first\n    batch = [\n        {\n            \"input_ids\": item[\"input_ids\"] if isinstance(item[\"input_ids\"], torch.Tensor)\n                        else torch.tensor(item[\"input_ids\"], dtype=torch.long),\n            \"attention_mask\": item[\"attention_mask\"] if isinstance(item[\"attention_mask\"], torch.Tensor)\n                              else torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n            \"labels\": item[\"labels\"] if isinstance(item[\"labels\"], torch.Tensor)\n                        else torch.tensor(item[\"labels\"], dtype=torch.long)\n        }\n        for item in batch\n    ]\n\n    # Now stack the tensors\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.stack([item[\"labels\"] for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef create_data_loaders(train_tokenized, dev_tokenized, test_tokenized):\n    train_loader = DataLoader(\n        HierarchicalDataset(train_tokenized),\n        batch_size=Config.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    dev_loader = DataLoader(\n        HierarchicalDataset(dev_tokenized),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    test_loader = DataLoader(\n        HierarchicalDataset(test_tokenized),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    return train_loader, dev_loader, test_loader\n\ndef compute_class_weights(train_hier, label2id):\n    label_counts = {label: 0 for label in label2id}\n    for example in train_hier:\n        for label in example['label']:\n            label_counts[label] += 1\n\n    total_samples = sum(label_counts.values())\n    weights = [\n        (total_samples / (label_counts[label] + 1e-5)) ** 0.5\n        for label in label2id\n    ]\n    weights = torch.tensor(weights, dtype=torch.float32)\n    return weights / weights.min()\n\ndef train_model(model, train_loader, dev_loader, optimizer, device, epochs, label_list):\n    \"\"\"Training loop with tensor validation\"\"\"\n    print(f\"\\n{'='*30} TRAINING STARTED {'='*30}\")\n    print(f\"Training on: {device}\")\n    print(f\"Number of epochs: {epochs}\")\n    print(f\"Batch size: {Config.batch_size}\")\n    print(f\"Learning rate: {Config.learning_rate}\")\n    print(f\"Total batches: {len(train_loader)}\")\n\n    # Create learning rate scheduler\n    total_steps = len(train_loader) * epochs // Config.gradient_accumulation_steps\n    warmup_steps = int(total_steps * Config.warmup_ratio)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n\n    model.train()\n    best_dev_f1 = 0\n    best_macro_f1 = 0\n    no_improve = 0\n    training_start = time.time()\n    history = []\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        optimizer.zero_grad()\n\n        # Training\n        model.train()\n        for batch_idx, batch in enumerate(train_loader):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            loss = outputs[\"loss\"] / Config.gradient_accumulation_steps\n            loss.backward()\n\n            total_loss += loss.item()\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=attention_mask[:, :, 0] > 0)\n\n            # Flatten predictions and labels\n            flat_labels = labels.cpu().numpy().flatten()\n            # Handle predictions correctly\n            flat_preds = []\n            for i, seq in enumerate(preds):\n                seq_len = (attention_mask[i, :, 0] > 0).sum().item()\n                flat_preds.extend(seq[:seq_len])\n            # Pad predictions to match labels length\n            flat_preds += [0] * (len(flat_labels) - len(flat_preds))\n            flat_preds = np.array(flat_preds)\n\n            all_preds.extend(flat_preds)\n            all_labels.extend(flat_labels)\n\n            # Gradient accumulation\n            if (batch_idx + 1) % Config.gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(train_loader)} | \"\n                      f\"Loss: {loss.item() * Config.gradient_accumulation_steps:.4f} | \"\n                      f\"Avg Loss: {total_loss/(batch_idx+1) * Config.gradient_accumulation_steps:.4f} | \"\n                      f\"LR: {scheduler.get_last_lr()[0]:.2e}\")\n\n        # Handle remaining gradients\n        if (batch_idx + 1) % Config.gradient_accumulation_steps != 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        epoch_time = time.time() - epoch_start\n        train_f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n\n        # Validation\n        model.eval()\n        dev_metrics = evaluate_metrics(model, dev_loader, device, label_list)\n        dev_f1 = dev_metrics[\"weighted_f1\"]\n        dev_macro = dev_metrics[\"macro_f1\"]\n\n        history.append({\n            'epoch': epoch+1,\n            'train_loss': total_loss/len(train_loader),\n            'train_f1': train_f1,\n            'dev_f1': dev_f1,\n            'dev_macro_f1': dev_macro\n        })\n\n        print(f\"\\nEpoch {epoch+1} completed in {epoch_time:.2f}s\")\n        print(f\"Train Loss: {total_loss/len(train_loader):.4f} | F1: {train_f1:.4f}\")\n        print(f\"Dev Weighted F1: {dev_f1:.4f} | Macro F1: {dev_macro:.4f}\")\n\n        # Early stopping check\n        if dev_f1 > best_dev_f1 or dev_macro > best_macro_f1:\n            if dev_f1 > best_dev_f1:\n                best_dev_f1 = dev_f1\n            if dev_macro > best_macro_f1:\n                best_macro_f1 = dev_macro\n            no_improve = 0\n            torch.save(model.state_dict(), os.path.join(Config.output_dir, \"best_model.pt\"))\n            print(f\"New best model saved with F1: {dev_f1:.4f}, Macro F1: {dev_macro:.4f}\")\n        else:\n            no_improve += 1\n            print(f\"No improvement for {no_improve}/{Config.patience} epochs\")\n            if no_improve >= Config.patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    training_time = time.time() - training_start\n    print(f\"Training completed in {training_time:.2f} seconds\")\n    print(f\"{'='*30} TRAINING COMPLETED {'='*30}\\n\")\n\n    # Load best model\n    model.load_state_dict(torch.load(os.path.join(Config.output_dir, \"best_model.pt\")))\n    return model, history\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Evaluation with tensor validation\"\"\"\n    model.eval()\n    all_preds, all_labels = [], []\n    total_time = 0\n    n_docs = 0\n    n_sentences = 0\n    eval_start = time.time()\n\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n            start = time.time()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            end = time.time()\n\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=mask)\n\n            # Filter out padded tokens\n            for i in range(len(labels)):\n                seq_len = mask[i].sum().item()\n                if seq_len > 0:\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n            total_time += (end - start)\n            n_docs += input_ids.shape[0]\n            n_sentences += mask.sum().item()\n\n    eval_end = time.time()\n    eval_time = eval_end - eval_start\n\n    # Classification report\n    if len(all_labels) > 0:\n        report = classification_report(\n            all_labels, all_preds,\n            labels=list(range(len(label_list))),\n            target_names=label_list,\n            output_dict=True,\n            zero_division=0\n        )\n\n        macro_f1 = report['macro avg']['f1-score']\n        weighted_f1 = report['weighted avg']['f1-score']\n        accuracy = report['accuracy']\n        per_label_f1 = {\n            label: report[label]['f1-score']\n            for label in label_list\n        }\n    else:\n        print(\"WARNING: No samples in evaluation\")\n        macro_f1 = 0\n        weighted_f1 = 0\n        accuracy = 0\n        per_label_f1 = {label: 0 for label in label_list}\n\n    # Calculate latency\n    latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n    latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n    return {\n        \"macro_f1\": macro_f1,\n        \"weighted_f1\": weighted_f1,\n        \"accuracy\": accuracy,\n        \"per_label_f1\": per_label_f1,\n        \"latency_ms_per_doc\": latency_doc,\n        \"latency_ms_per_sentence\": latency_sent,\n        \"eval_time_seconds\": eval_time,\n        \"num_samples\": n_docs\n    }\n\ndef get_model_size_mb(model):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    return (param_size + buffer_size) / (1024 ** 2)\n\ndef get_memory_footprint():\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / (1024 ** 3)\n\ndef save_checkpoint(model, tokenizer, metrics, label2id, config, save_dir, checkpoint_name):\n    \"\"\"Save model checkpoint with all artifacts\"\"\"\n    checkpoint_path = os.path.join(save_dir, checkpoint_name)\n    os.makedirs(checkpoint_path, exist_ok=True)\n\n    # Save model\n    torch.save(model.state_dict(), os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n\n    # Save tokenizer\n    tokenizer.save_pretrained(checkpoint_path)\n\n    # Save adapter configuration separately\n    model.bert.save_pretrained(checkpoint_path)  # Save LoRA adapter\n\n    # Save metadata\n    with open(os.path.join(checkpoint_path, \"config.json\"), \"w\") as f:\n        json.dump({\n            \"label2id\": label2id,\n            \"id2label\": {i: l for l, i in label2id.items()},\n            \"model_config\": {\n                \"bert_model_name\": config.bert_model_name,\n                \"max_num_sentences\": config.max_num_sentences,\n                \"max_length\": config.max_length,\n                \"dropout_rate\": config.dropout_rate,\n                \"gamma\": config.gamma,\n                \"lora_r\": config.adalora_init_r,  # Updated to AdaLoRA params\n                \"lora_alpha\": config.lora_alpha,\n                \"lora_dropout\": config.lora_dropout,\n                \"lora_target_modules\": config.lora_target_modules,\n                \"adalora_target_r\": config.adalora_target_r\n            }\n        }, f, indent=2)\n\n    # Save metrics\n    with open(os.path.join(checkpoint_path, \"metrics.json\"), \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    print(f\"Checkpoint saved to {checkpoint_path}\")\n    return checkpoint_path\n\ndef upload_to_huggingface(save_path, repo_id):\n    \"\"\"Upload model to Hugging Face Hub\"\"\"\n    create_repo(repo_id, exist_ok=True, token=True)\n    upload_folder(\n        repo_id=repo_id,\n        folder_path=save_path,\n        commit_message=\"Hierarchical Legal Model with AdaLoRA (Final Fixed)\",\n        repo_type=\"model\",\n        token=True\n    )\n    print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n\ndef main():\n    \"\"\"Main pipeline with enhanced error handling\"\"\"\n    try:\n        # Setup\n        start_time = time.time()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(f\"\\n{'='*50}\")\n        print(f\"FINAL FIXED HIERARCHICAL LEGAL MODEL WITH ADALORA\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"{'='*50}\\n\")\n\n        # Create output directory\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        # Phase 1: Data Preparation\n        print(\"Preparing datasets...\")\n        train_hier, dev_hier, test_hier, label2id, id2label, label_list = prepare_hierarchical_datasets(\n            train_ds, dev_ds, test_ds\n        )\n\n        # Compute class weights\n        print(\"Computing class weights...\")\n        class_weights = compute_class_weights(train_hier, label2id).to(device)\n        print(f\"Class weights: {class_weights.cpu().numpy()}\")\n\n        # Phase 2: Tokenization\n        print(\"Tokenizing datasets...\")\n        train_tokenized, dev_tokenized, test_tokenized, tokenizer = tokenize_datasets(\n            train_hier, dev_hier, test_hier, label2id\n        )\n        print(\"Creating data loaders...\")\n        train_loader, dev_loader, test_loader = create_data_loaders(\n            train_tokenized, dev_tokenized, test_tokenized\n        )\n        \n        # Calculate total steps for AdaLoRA\n        total_steps = len(train_loader) * Config.epochs // Config.gradient_accumulation_steps\n        print(f\"Total training steps: {total_steps}\")\n\n        # Phase 3: Model Initialization\n        print(\"Initializing model...\")\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            total_steps=total_steps,  # Pass total_steps here\n            class_weights=class_weights\n        ).to(device)\n\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=Config.learning_rate,\n            weight_decay=Config.weight_decay\n        )\n\n        # Phase 4: Training\n        print(\"Starting training...\")\n        model, history = train_model(\n            model, train_loader, dev_loader,\n            optimizer, device, Config.epochs, label_list\n        )\n\n        # Phase 5: Evaluation\n        print(\"\\nEvaluating on dev set...\")\n        dev_metrics = evaluate_metrics(model, dev_loader, device, label_list)\n        \n        print(\"\\nEvaluating on train set...\")\n        train_metrics = evaluate_metrics(model, train_loader, device, label_list)\n\n        # Phase 6: Metrics Collection\n        metrics = {\n            \"train\": train_metrics,  # Include train metrics\n            \"dev\": dev_metrics,\n            \"model_size_mb\": get_model_size_mb(model),\n            \"training_memory_footprint_gb\": get_memory_footprint(),\n            \"label2id\": label2id,\n            \"id2label\": id2label,\n            \"training_time\": time.time() - start_time,\n            \"training_history\": history\n        }\n\n        # Phase 7: Save and Upload\n        checkpoint_path = save_checkpoint(\n            model, tokenizer, metrics, label2id, Config,\n            Config.output_dir, Config.save_checkpoint\n        )\n        upload_to_huggingface(checkpoint_path, Config.hf_repo_id)\n\n        # Final Report\n        print(\"\\n==== FINAL METRICS ====\")\n        print(\"TRAIN SET:\")\n        print(f\"  Weighted F1:   {train_metrics['weighted_f1']:.4f}\")\n        print(f\"  Macro F1:      {train_metrics['macro_f1']:.4f}\")\n        print(f\"  Accuracy:      {train_metrics['accuracy']:.4f}\")\n        \n        print(\"\\nDEV SET:\")\n        print(f\"  Weighted F1:   {dev_metrics['weighted_f1']:.4f}\")\n        print(f\"  Macro F1:      {dev_metrics['macro_f1']:.4f}\")\n        print(f\"  Accuracy:      {dev_metrics['accuracy']:.4f}\")\n        \n        print(f\"\\nModel Size:        {metrics['model_size_mb']:.2f} MB\")\n        print(f\"Training Time:     {metrics['training_time']:.2f} seconds\")\n        print(f\"Saved to:          {checkpoint_path}\")\n\n        # Print per-class F1 scores\n        print(\"\\nPer-class F1 Scores (Train Set):\")\n        for label, score in train_metrics[\"per_label_f1\"].items():\n            print(f\"{label}: {score:.4f}\")\n            \n        print(\"\\nPer-class F1 Scores (Dev Set):\")\n        for label, score in dev_metrics[\"per_label_f1\"].items():\n            print(f\"{label}: {score:.4f}\")\n\n        print(f\"\\n{'='*50}\")\n        print(\"TRAINING PIPELINE COMPLETED SUCCESSFULLY\")\n        print(f\"{'='*50}\")\n\n        return metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"PIPELINE FAILED!\")\n        print(f\"Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        print(f\"{'!'*50}\")\n        # Save error log\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Pipeline error at {datetime.now()}\\n\")\n            f.write(str(e))\n            f.write(traceback.format_exc())\n        return None\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}