{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn.functional as F\nfrom datasets import load_dataset\nfrom huggingface_hub import notebook_login, create_repo, upload_folder\nimport pandas as pd\nfrom datasets import Dataset\nimport torch\nimport time\nimport os\nimport json\nimport psutil\nimport numpy as np\nimport math\nfrom datetime import datetime\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nimport torch.nn as nn\nfrom transformers import AutoModel\nfrom torchcrf import CRF\nimport random\nfrom collections import Counter\nfrom datasets import concatenate_datasets\nimport nltk\nfrom nltk.corpus import wordnet\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import BitsAndBytesConfig\nfrom accelerate import Accelerator\nimport traceback\n\n# Download required NLTK data\nnltk.download('wordnet', quiet=True)\nnltk.download('omw-1.4', quiet=True)\nnltk.download('punkt_tab')\n\nclass Config:\n    # Model hyperparameters\n    bert_model_name = 'nlpaueb/legal-bert-base-uncased'\n    lstm_hidden_size = 200\n    context_hidden_size = 200\n    max_num_sentences = 32\n    max_length = 64\n    dropout_rate = 0.4\n    gamma = 2.0\n    weight_decay = 1e-5\n\n    # FSSA parameters - reduced ranks and increased sparsity\n    fssa_linear_rank = 1   # Reduced from 4\n    fssa_emb_rank = 1      # Reduced from 2\n    fssa_linear_sparsity = 0.99  # Increased sparsity from 0.92\n    fssa_emb_sparsity = 0.995\n    fssa_block_size = 32\n\n    # New parameters for size reduction\n    context_intermediate_size = 380  # Reduced from 2000\n    emission_hidden_size = 64        # Reduced from 384\n\n    # QLoRA parameters\n    qlora_r = 8\n    qlora_alpha = 32\n    qlora_dropout = 0.05\n    qlora_target_modules = [\"query\", \"key\", \"value\", \"dense\"]\n    qlora_compute_dtype = torch.bfloat16\n\n    # Training parameters\n    epochs = 8\n    batch_size = 4\n    learning_rate = 5e-5\n    warmup_ratio = 0.1\n\n    # Paths and repo info\n    hf_repo_id = \"Please enter your huggingface user id here/hierarchical-legal-model-improved-fssa-qlora\"\n    output_dir = \"./improved_hierarchical_model_fssa_qlora\"\n    save_checkpoint = \"best_model\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to Hugging Face Hub\nnotebook_login()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import notebook_login\n\n# Load datasets\nsplits = {\n    'train': 'data/train-00000-of-00001-bb0092e0d8549337.parquet',\n    'dev': 'data/dev-00000-of-00001-af55705c75623915.parquet',\n    'test': 'data/test-00000-of-00001-2526ab833e27e0ee.parquet'\n}\n\ntrain_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"train\"])\ndev_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"dev\"])\ntest_df = pd.read_parquet(\"hf://datasets/opennyaiorg/InRhetoricalRoles/\" + splits[\"test\"])\n\n# Convert to Hugging Face Datasets\ntrain_ds = Dataset.from_pandas(train_df)\ndev_ds = Dataset.from_pandas(dev_df)\ntest_ds = Dataset.from_pandas(test_df)\n\ndef get_synonyms(word):\n    \"\"\"Get synonyms for data augmentation\"\"\"\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonym = lemma.name().replace(\"_\", \" \").lower()\n            if synonym != word and len(synonym) > 1:\n                synonyms.add(synonym)\n    return list(synonyms) if synonyms else [word]\n\ndef augment_sentence(sent):\n    \"\"\"Enhanced data augmentation with synonym replacement\"\"\"\n    if len(sent.strip()) == 0:\n        return sent\n\n    words = nltk.word_tokenize(sent)\n    if len(words) < 2:\n        return sent\n\n    # Choose an augmentation technique\n    choice = random.choices([1, 2, 3, 4], weights=[0.4, 0.2, 0.3, 0.1])[0]\n\n    if choice == 1:  # Synonym replacement\n        idx = random.randint(0, len(words)-1)\n        synonyms = get_synonyms(words[idx])\n        if synonyms and len(synonyms) > 0:\n            words[idx] = random.choice(synonyms)\n    elif choice == 2 and len(words) >= 4:  # Random deletion\n        del_idx = random.randint(0, len(words)-1)\n        del words[del_idx]\n    elif choice == 3 and len(words) >= 3:  # Word swap\n        i, j = random.sample(range(len(words)), 2)\n        words[i], words[j] = words[j], words[i]\n    elif choice == 4:  # Random insertion\n        idx = random.randint(0, len(words)-1)\n        synonyms = get_synonyms(words[idx])\n        if synonyms and len(synonyms) > 0:\n            words.insert(idx, random.choice(synonyms))\n\n    return \" \".join(words)\n\ndef prepare_hierarchical_datasets(train_ds, dev_ds, test_ds):\n    \"\"\"Optimized dataset preparation with efficient sampling\"\"\"\n    print(\"Preprocessing datasets with efficient sampling...\")\n\n    def get_spans_and_labels(example):\n        spans = []\n        labels = []\n        if example.get('annotations') and len(example['annotations']) > 0:\n            if example['annotations'][0].get('result'):\n                for ann in example['annotations'][0]['result']:\n                    if ann.get('value') and ann['value'].get('text') and ann['value'].get('labels'):\n                        spans.append(ann['value']['text'])\n                        labels.append(ann['value']['labels'][0])\n        return {'spans': spans, 'labels': labels}\n\n    # Apply to all splits\n    train_ds = train_ds.map(get_spans_and_labels)\n    dev_ds = dev_ds.map(get_spans_and_labels)\n    test_ds = test_ds.map(get_spans_and_labels)\n\n    # Filter out empty examples\n    train_ds = train_ds.filter(lambda x: len(x['spans']) > 0)\n    dev_ds = dev_ds.filter(lambda x: len(x['spans']) > 0)\n    test_ds = test_ds.filter(lambda x: len(x['spans']) > 0)\n\n    def prepare_for_hierarchical(example):\n        return {'text': example['spans'], 'label': example['labels']}\n\n    train_hier = train_ds.map(prepare_for_hierarchical)\n    dev_hier = dev_ds.map(prepare_for_hierarchical)\n    test_hier = test_ds.map(prepare_for_hierarchical)\n\n    # Build label mapping\n    all_labels = set()\n    for example in train_hier:\n        all_labels.update(example['label'])\n    label_list = sorted(list(all_labels))\n    label2id = {l: i for i, l in enumerate(label_list)}\n    id2label = {i: l for i, l in enumerate(label_list)}\n\n    print(f\"Identified {len(label_list)} labels: {label_list}\")\n\n    # Efficient data augmentation (only for rare classes)\n    def augment_dataset(dataset, label2id):\n        label_counts = Counter()\n        for example in dataset:\n            label_counts.update(example['label'])\n\n        # Identify rare classes\n        rare_classes = [label for label, count in label_counts.items() if count < 10]\n        print(f\"Rare classes (<10 samples): {rare_classes}\")\n\n        augmented_examples = []\n        for example in dataset:\n            labels = example['label']\n            copies = 1\n\n            if any(label in rare_classes for label in labels):\n                copies = 3  # Moderate augmentation for rare classes\n\n            # Create augmented copies\n            for _ in range(copies):\n                augmented_text = [\n                    augment_sentence(sent) if random.random() < 0.5 and sent.strip() else sent\n                    for sent in example['text']\n                ]\n                augmented_examples.append({\n                    'text': augmented_text,\n                    'label': labels.copy()\n                })\n\n        print(f\"Added {len(augmented_examples)} augmented examples\")\n        return concatenate_datasets([dataset, Dataset.from_list(augmented_examples)])\n\n    # Apply augmentation to training set\n    train_hier = augment_dataset(train_hier, label2id)\n\n    # Efficient class balancing\n    label_counts = Counter()\n    for example in train_hier:\n        label_counts.update(example['label'])\n\n    # Calculate target counts - cap at 1000 samples per class\n    max_count = min(1000, max(label_counts.values()))\n    balanced_examples = []\n    for label in label_list:\n        # Collect examples containing this label\n        class_examples = [ex for ex in train_hier if label in ex['label']]\n        current_count = label_counts[label]\n\n        # Calculate how many to add\n        needed = max(0, max_count - current_count)\n\n        # If we need to add examples, duplicate existing ones\n        if needed > 0 and class_examples:\n            # Add existing examples\n            balanced_examples.extend(class_examples)\n            # Add duplicated examples\n            duplicates = min(needed, len(class_examples))\n            balanced_examples.extend(random.choices(class_examples, k=duplicates))\n        else:\n            balanced_examples.extend(class_examples)\n\n    # Create balanced dataset\n    train_hier = Dataset.from_list(balanced_examples)\n    print(f\"Created balanced dataset with {len(train_hier)} examples\")\n\n    return train_hier, dev_hier, test_hier, label2id, id2label, label_list\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional embeddings for sentence order\"\"\"\n    def __init__(self, d_model, max_len=Config.max_num_sentences):\n        super().__init__()\n        self.position_emb = nn.Embedding(max_len, d_model)\n\n    def forward(self, x):\n        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)\n        return x + self.position_emb(positions)\n\nclass TransformerContextLayer(nn.Module):\n    \"\"\"Transformer-based context modeling with reduced FFN size\"\"\"\n    def __init__(self, d_model, nhead=4, dim_feedforward=Config.context_intermediate_size, dropout=0.1):\n        super().__init__()\n        self.encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=1)\n\n    def forward(self, x):\n        return self.transformer_encoder(x)\n\nclass EmissionLayer(nn.Module):\n    \"\"\"Emission layer with reduced hidden size\"\"\"\n    def __init__(self, input_size, num_labels, dropout=0.2):\n        super().__init__()\n        self.mlp = nn.Sequential(\n            nn.Linear(input_size, Config.emission_hidden_size),  # Reduced hidden size\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(Config.emission_hidden_size, num_labels)   # Final layer\n        )\n\n    def forward(self, x):\n        return self.mlp(x)\n\nclass FocalCRF(nn.Module):\n    \"\"\"CRF with focal loss for class imbalance\"\"\"\n    def __init__(self, num_tags, gamma=Config.gamma):\n        super().__init__()\n        self.crf = CRF(num_tags, batch_first=True)\n        self.gamma = gamma\n\n    def forward(self, emissions, tags, mask, class_weights=None):\n        # Compute standard CRF loss\n        log_likelihood = self.crf(emissions, tags, mask=mask, reduction='none')\n\n        # Apply focal loss transformation\n        pt = torch.exp(log_likelihood)\n        focal_loss = -((1 - pt) ** self.gamma) * log_likelihood\n\n        # Apply class weights if provided\n        if class_weights is not None:\n            weights_per_tag = class_weights[tags]  # (batch_size, seq_len)\n            valid_counts = mask.sum(dim=1)  # (batch_size,)\n            weights_per_sequence = weights_per_tag.sum(dim=1) / valid_counts\n            focal_loss = focal_loss * weights_per_sequence\n\n        return focal_loss.mean()\n\n    def decode(self, emissions, mask):\n        return self.crf.decode(emissions, mask=mask)\n\n# ====================================================\n# FSSA: Factorized Structured Sparse Adaptation Layers\n# ====================================================\nclass FSSALayer(nn.Module):\n    \"\"\"Factorized Structured Sparse Adaptation layer with higher sparsity\"\"\"\n    def __init__(self, original_layer, rank=Config.fssa_linear_rank,\n                 sparsity=Config.fssa_linear_sparsity, block_size=Config.fssa_block_size):\n        super().__init__()\n        self.original_layer = original_layer\n        self.rank = rank\n        self.sparsity = sparsity\n        self.block_size = block_size\n\n        # Freeze original parameters\n        for param in self.original_layer.parameters():\n            param.requires_grad = False\n\n        in_features = original_layer.in_features\n        out_features = original_layer.out_features\n\n        # Factorized sparse adaptation parameters\n        self.A = nn.Parameter(torch.zeros(rank, in_features))\n        self.B = nn.Parameter(torch.zeros(out_features, rank))\n\n        # Structured sparsity mask\n        self.mask = self.create_sparsity_mask(out_features, in_features)\n\n        # Initialize parameters\n        self.init_parameters()\n\n    def create_sparsity_mask(self, rows, cols):\n        # Create block-sparse mask\n        row_blocks = (rows + self.block_size - 1) // self.block_size\n        col_blocks = (cols + self.block_size - 1) // self.block_size\n\n        # Generate sparse block pattern\n        num_blocks = row_blocks * col_blocks\n        num_active = int(num_blocks * (1 - self.sparsity))\n        active_blocks = random.sample(range(num_blocks), num_active)\n\n        # Create full mask\n        mask = torch.zeros(rows, cols)\n        for block_idx in active_blocks:\n            i = block_idx // col_blocks\n            j = block_idx % col_blocks\n            row_start = i * self.block_size\n            col_start = j * self.block_size\n            row_end = min(row_start + self.block_size, rows)\n            col_end = min(col_start + self.block_size, cols)\n            mask[row_start:row_end, col_start:col_end] = 1\n\n        return mask\n\n    def init_parameters(self):\n        # Initialize with small random values\n        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.B, a=math.sqrt(5))\n\n    def forward(self, x):\n        # Original layer output\n        base_output = self.original_layer(x)\n\n        # Factorized sparse adaptation\n        adapted = (self.B @ self.A) * self.mask.to(self.B.device)\n\n        # FIX: Remove the .t() transpose - use adapted directly\n        delta_output = F.linear(x, adapted)  # CHANGED: removed .t()\n\n        return base_output + delta_output\n\nclass FSSAEmbedding(nn.Module):\n    \"\"\"Factorized Structured Sparse Adaptation for embeddings\"\"\"\n    def __init__(self, original_embedding, rank=Config.fssa_emb_rank,\n                 sparsity=Config.fssa_emb_sparsity):\n        super().__init__()\n        self.original_embedding = original_embedding\n        self.rank = rank\n        self.sparsity = sparsity\n\n        # Freeze original parameters\n        for param in self.original_embedding.parameters():\n            param.requires_grad = False\n\n        num_embeddings = original_embedding.num_embeddings\n        embedding_dim = original_embedding.embedding_dim\n\n        # Factorized adaptation parameters\n        self.U = nn.Parameter(torch.zeros(num_embeddings, rank))\n        self.V = nn.Parameter(torch.zeros(rank, embedding_dim))\n\n        # Sparsity mask\n        self.mask = (torch.rand(num_embeddings, rank) > sparsity).float()\n\n        # Initialize parameters\n        self.init_parameters()\n\n    def init_parameters(self):\n        nn.init.normal_(self.U, mean=0, std=0.02)\n        nn.init.normal_(self.V, mean=0, std=0.02)\n\n    def forward(self, input_ids):\n        base_embeds = self.original_embedding(input_ids)\n\n        # Compute adaptation (sparse factorized)\n        adapted = (self.U * self.mask.to(self.U.device)) @ self.V\n        # Lookup adaptation vectors\n        delta_embeds = F.embedding(input_ids, adapted)\n\n        return base_embeds + delta_embeds\n\ndef apply_fssa_to_hierarchical(model):\n    \"\"\"Apply FSSA only to hierarchical components (skip BERT)\"\"\"\n    # Apply FSSA to positional embeddings\n    model.position_enc.position_emb = FSSAEmbedding(\n        model.position_enc.position_emb\n    )\n    \n    # Apply FSSA to transformer context encoder\n    for name, module in model.context_encoder.named_children():\n        if isinstance(module, nn.Linear):\n            setattr(model.context_encoder, name, FSSALayer(module))\n        else:\n            # Recursively apply to submodules\n            for sub_name, sub_module in module.named_children():\n                if isinstance(sub_module, nn.Linear):\n                    setattr(module, sub_name, FSSALayer(sub_module))\n    \n    # Apply FSSA to emission layer\n    for i, layer in enumerate(model.emission.mlp):\n        if isinstance(layer, nn.Linear):\n            model.emission.mlp[i] = FSSALayer(layer)\n    \n    return model\n\ndef count_parameters(model):\n    \"\"\"Count trainable parameters\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef print_parameter_counts(model):\n    \"\"\"Print parameter statistics\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = count_parameters(model)\n\n    print(\"=\"*50)\n    print(f\"TOTAL PARAMETERS: {total_params/1e6:.2f}M\")\n    print(f\"TRAINABLE PARAMETERS: {trainable_params/1e6:.2f}M\")\n    print(\"=\"*50)\n\n    return trainable_params\n\nclass ImprovedHSLNModel(nn.Module):\n    \"\"\"Hybrid QLoRA (BERT) + FSSA (Hierarchical) Model\"\"\"\n    def __init__(self, num_labels, class_weights=None):\n        super().__init__()\n        self.class_weights = class_weights\n\n        # Configure 4-bit quantization for BERT\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=Config.qlora_compute_dtype\n        )\n        \n        # Load BERT with QLoRA\n        self.bert = AutoModel.from_pretrained(\n            Config.bert_model_name,\n            quantization_config=bnb_config\n        )\n        \n        # Prepare model for k-bit training\n        self.bert = prepare_model_for_kbit_training(self.bert)\n        \n        # Apply QLoRA adapters\n        lora_config = LoraConfig(\n            r=Config.qlora_r,\n            lora_alpha=Config.qlora_alpha,\n            target_modules=Config.qlora_target_modules,\n            lora_dropout=Config.qlora_dropout,\n            bias=\"none\",\n            task_type=\"FEATURE_EXTRACTION\"\n        )\n        self.bert = get_peft_model(self.bert, lora_config)\n\n        # Hierarchical components\n        self.sent_dropout = nn.Dropout(Config.dropout_rate)\n        self.sent_layer_norm = nn.LayerNorm(self.bert.config.hidden_size)\n        self.position_enc = PositionalEncoding(self.bert.config.hidden_size)\n        self.context_encoder = TransformerContextLayer(\n            d_model=self.bert.config.hidden_size\n        )\n        self.emission = EmissionLayer(\n            input_size=self.bert.config.hidden_size,\n            num_labels=num_labels\n        )\n        \n        # Apply FSSA to hierarchical components (skip BERT)\n        self = apply_fssa_to_hierarchical(self)\n        \n        # CRF layer with focal loss\n        self.crf = FocalCRF(num_labels, gamma=Config.gamma)\n        \n        # Print entire model parameter counts\n        print_parameter_counts(self)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        batch_size, num_sent, seq_len = input_ids.shape\n\n        # Process each sentence\n        flat_input_ids = input_ids.view(-1, seq_len)\n        flat_mask = attention_mask.view(-1, seq_len)\n\n        bert_out = self.bert(\n            input_ids=flat_input_ids,\n            attention_mask=flat_mask\n        ).last_hidden_state\n\n        # Sentence embeddings (CLS token)\n        sent_emb = bert_out[:, 0, :]\n        sent_emb = self.sent_layer_norm(sent_emb)\n        sent_emb = self.sent_dropout(sent_emb)\n        sent_emb = sent_emb.view(batch_size, num_sent, -1)\n\n        # Context modeling\n        sent_emb = self.position_enc(sent_emb)\n        context_emb = self.context_encoder(sent_emb)\n\n        # Emissions\n        emissions = self.emission(context_emb)\n        mask = attention_mask[:, :, 0] > 0  # Sentence-level mask\n\n        if labels is not None:\n            loss = self.crf(\n                emissions,\n                labels,\n                mask=mask,\n                class_weights=self.class_weights\n            )\n            return {\"loss\": loss, \"emissions\": emissions}\n        return {\"emissions\": emissions}\n\ndef tokenize_datasets(train_hier, dev_hier, test_hier, label2id):\n    \"\"\"Tokenize datasets for hierarchical input\"\"\"\n    print(\"Tokenizing datasets...\")\n    tokenizer = AutoTokenizer.from_pretrained(Config.bert_model_name)\n\n    def tokenize_document(example):\n        sentences = example['text']\n        labels = example['label']\n        sentences = sentences[:Config.max_num_sentences]\n        labels = labels[:Config.max_num_sentences]\n        pad_len = Config.max_num_sentences - len(sentences)\n        sentences += [\"\"] * pad_len\n        labels += [list(label2id.keys())[0]] * pad_len\n\n        input_ids = []\n        attention_mask = []\n        for sent in sentences:\n            encoded = tokenizer(\n                sent,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=Config.max_length,\n                return_tensors=\"pt\"\n            )\n            input_ids.append(encoded[\"input_ids\"].squeeze(0))\n            attention_mask.append(encoded[\"attention_mask\"].squeeze(0))\n\n        input_ids = torch.stack(input_ids)\n        attention_mask = torch.stack(attention_mask)\n        label_ids = torch.tensor([label2id[l] for l in labels])\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": label_ids\n        }\n\n    train_tokenized = train_hier.map(tokenize_document)\n    dev_tokenized = dev_hier.map(tokenize_document)\n    test_tokenized = test_hier.map(tokenize_document)\n\n    return train_tokenized, dev_tokenized, test_tokenized, tokenizer\n\nclass HierarchicalDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        return {\n            \"input_ids\": item[\"input_ids\"],\n            \"attention_mask\": item[\"attention_mask\"],\n            \"labels\": item[\"labels\"]\n        }\n\ndef collate_fn(batch):\n    def ensure_tensor(x):\n        return torch.tensor(x) if not isinstance(x, torch.Tensor) else x\n\n    input_ids = torch.stack([ensure_tensor(item[\"input_ids\"]) for item in batch])\n    attention_mask = torch.stack([ensure_tensor(item[\"attention_mask\"]) for item in batch])\n    labels = torch.stack([ensure_tensor(item[\"labels\"]) for item in batch])\n\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\ndef create_data_loaders(train_tokenized, dev_tokenized, test_tokenized):\n    train_loader = DataLoader(\n        HierarchicalDataset(train_tokenized),\n        batch_size=Config.batch_size,\n        shuffle=True,\n        collate_fn=collate_fn\n    )\n    dev_loader = DataLoader(\n        HierarchicalDataset(dev_tokenized),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    test_loader = DataLoader(\n        HierarchicalDataset(test_tokenized),\n        batch_size=Config.batch_size,\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    return train_loader, dev_loader, test_loader\n\ndef compute_class_weights(train_hier, label2id):\n    \"\"\"Compute linear class weights without squaring\"\"\"\n    label_counts = {label: 0 for label in label2id}\n    for example in train_hier:\n        for label in example['label']:\n            label_counts[label] += 1\n\n    total_samples = sum(label_counts.values())\n    weights = [\n        total_samples / (label_counts[label] + 1e-5)  # Linear weighting\n        for label in label2id\n    ]\n    weights = torch.tensor(weights, dtype=torch.float32)\n    return weights / weights.min()  # Normalize\n\ndef train_model(model, train_loader, dev_loader, optimizer, device, epochs, label_list):\n    \"\"\"Training loop with learning rate warmup\"\"\"\n    print(f\"\\n{'='*30} TRAINING STARTED {'='*30}\")\n    print(f\"Training on: {device}\")\n    print(f\"Number of epochs: {epochs}\")\n    print(f\"Batch size: {Config.batch_size}\")\n    print(f\"Learning rate: {Config.learning_rate}\")\n    print(f\"Total batches: {len(train_loader)}\")\n\n    model.train()\n    best_dev_f1 = 0\n    training_start = time.time()\n    history = []\n\n    # Warmup scheduling\n    total_steps = epochs * len(train_loader)\n    warmup_steps = int(total_steps * Config.warmup_ratio)\n\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        total_loss = 0\n        all_preds, all_labels = [], []\n        step = 0\n\n        # Training\n        model.train()\n        for batch_idx, batch in enumerate(train_loader):\n            current_step = epoch * len(train_loader) + batch_idx\n\n            # Learning rate warmup\n            if current_step < warmup_steps:\n                lr_scale = min(1.0, float(current_step + 1) / warmup_steps)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = Config.learning_rate * lr_scale\n\n            optimizer.zero_grad()\n\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            loss = outputs[\"loss\"]\n            loss.backward()\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n\n            total_loss += loss.item()\n            emissions = outputs[\"emissions\"]\n            preds = model.crf.decode(emissions, mask=attention_mask[:, :, 0] > 0)\n\n            # Flatten predictions and labels\n            flat_labels = labels.cpu().numpy().flatten()\n            flat_preds = np.array([p for seq in preds for p in seq] +\n                                  [0]*(len(flat_labels) - sum(len(seq) for seq in preds)))\n\n            all_preds.extend(flat_preds)\n            all_labels.extend(flat_labels)\n\n            if (batch_idx + 1) % 10 == 0:\n                avg_loss = total_loss/(batch_idx+1)\n                print(f\"Epoch {epoch+1}/{epochs} | Batch {batch_idx+1}/{len(train_loader)} | \"\n                      f\"Loss: {loss.item():.4f} | Avg Loss: {avg_loss:.4f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        epoch_time = time.time() - epoch_start\n        train_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n        train_acc = accuracy_score(all_labels, all_preds)\n\n        # Validation\n        model.eval()\n        dev_metrics = evaluate_metrics(model, dev_loader, device, label_list)\n        dev_f1 = dev_metrics[\"weighted_f1\"]\n\n        history.append({\n            'epoch': epoch+1,\n            'train_loss': total_loss/len(train_loader),\n            'train_f1': train_f1,\n            'dev_f1': dev_f1\n        })\n\n        print(f\"\\nEpoch {epoch+1} completed in {epoch_time:.2f}s\")\n        print(f\"Train Loss: {total_loss/len(train_loader):.4f} | F1: {train_f1:.4f}\")\n        print(f\"Dev Weighted F1: {dev_f1:.4f} | Macro F1: {dev_metrics['macro_f1']:.4f}\")\n\n        # Save best model without quantization parameters\n        if dev_f1 > best_dev_f1:\n            best_dev_f1 = dev_f1\n            \n            # Create filtered state_dict\n            filtered_state_dict = {}\n            for name, param in model.state_dict().items():\n                if not any(q in name for q in ['quant_state', 'absmax', 'quant_map', 'nested_absmax', 'nested_quant_map']):\n                    filtered_state_dict[name] = param\n            \n            torch.save(filtered_state_dict, os.path.join(Config.output_dir, \"best_model.pt\"))\n            print(f\"New best model saved with F1: {dev_f1:.4f}\")\n\n    training_time = time.time() - training_start\n    print(f\"Training completed in {training_time:.2f} seconds\")\n    print(f\"{'='*30} TRAINING COMPLETED {'='*30}\\n\")\n\n    # Load best model with safe loading\n    try:\n        best_model_path = os.path.join(Config.output_dir, \"best_model.pt\")\n        if os.path.exists(best_model_path):\n            checkpoint = torch.load(best_model_path, map_location=device)\n            \n            # Filter out any quantization parameters that might have been saved\n            filtered_checkpoint = {}\n            for name, param in checkpoint.items():\n                if not any(q in name for q in ['quant_state', 'absmax', 'quant_map', 'nested_absmax', 'nested_quant_map']):\n                    filtered_checkpoint[name] = param\n            \n            model.load_state_dict(filtered_checkpoint, strict=False)\n            print(\"Loaded best model checkpoint\")\n        else:\n            print(\"Best model checkpoint not found. Using final model state.\")\n    except Exception as e:\n        print(f\"Error loading best model: {str(e)}\")\n        print(\"Using final model state for evaluation\")\n\n    return model, history\n\ndef evaluate_metrics(model, dataloader, device, label_list):\n    \"\"\"Comprehensive evaluation with padding masking\"\"\"\n    try:\n        model.eval()\n        all_preds, all_labels = [], []\n        total_time = 0\n        n_docs = 0\n        n_sentences = 0\n        eval_start = time.time()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                mask = attention_mask[:, :, 0] > 0\n\n                start = time.time()\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n                end = time.time()\n\n                emissions = outputs[\"emissions\"]\n                preds = model.crf.decode(emissions, mask=mask)\n\n                for i in range(len(labels)):\n                    seq_len = mask[i].sum().item()\n                    all_preds.extend(preds[i][:seq_len])\n                    all_labels.extend(labels[i][:seq_len].cpu().numpy())\n\n                total_time += (end - start)\n                n_docs += input_ids.shape[0]\n                n_sentences += mask.sum().item()\n\n        eval_end = time.time()\n        eval_time = eval_end - eval_start\n\n        labels_for_report = list(range(len(label_list)))\n        target_names = label_list\n\n        report = classification_report(\n            all_labels, all_preds,\n            labels=labels_for_report,\n            target_names=target_names,\n            output_dict=True,\n            zero_division=0\n        )\n\n        macro_f1 = report['macro avg']['f1-score']\n        weighted_f1 = report['weighted avg']['f1-score']\n        accuracy = accuracy_score(all_labels, all_preds)\n        per_label_f1 = {\n            label: report[label]['f1-score']\n            for label in label_list\n        }\n\n        latency_doc = (total_time / n_docs) * 1000 if n_docs else 0\n        latency_sent = (total_time / n_sentences) * 1000 if n_sentences else 0\n\n        return {\n            \"macro_f1\": macro_f1,\n            \"weighted_f1\": weighted_f1,\n            \"accuracy\": accuracy,\n            \"per_label_f1\": per_label_f1,\n            \"latency_ms_per_doc\": latency_doc,\n            \"latency_ms_per_sentence\": latency_sent,\n            \"eval_time_seconds\": eval_time,\n            \"num_samples\": n_docs\n        }\n\n    except Exception as e:\n        print(f\"Evaluation failed: {str(e)}\")\n        raise\n\ndef get_model_size_mb(model):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    return (param_size + buffer_size) / (1024 ** 2)\n\ndef get_memory_footprint():\n    process = psutil.Process(os.getpid())\n    return process.memory_info().rss / (1024 ** 3)\n\ndef save_checkpoint(model, tokenizer, metrics, label2id, config, save_dir, checkpoint_name):\n    \"\"\"Save model checkpoint without quantization state parameters\"\"\"\n    try:\n        checkpoint_path = os.path.join(save_dir, checkpoint_name)\n        os.makedirs(checkpoint_path, exist_ok=True)\n        \n        # Create filtered state_dict without quantization parameters\n        filtered_state_dict = {}\n        for name, param in model.state_dict().items():\n            if not any(q in name for q in ['quant_state', 'absmax', 'quant_map', 'nested_absmax', 'nested_quant_map']):\n                filtered_state_dict[name] = param\n                \n        torch.save(filtered_state_dict, os.path.join(checkpoint_path, \"pytorch_model.bin\"))\n        tokenizer.save_pretrained(checkpoint_path)\n\n        with open(os.path.join(checkpoint_path, \"config.json\"), \"w\") as f:\n            json.dump({\n                \"label2id\": label2id,\n                \"id2label\": {i: l for l, i in label2id.items()},\n                \"model_config\": {\n                    \"bert_model_name\": config.bert_model_name,\n                    \"lstm_hidden_size\": config.lstm_hidden_size,\n                    \"context_hidden_size\": config.context_hidden_size,\n                    \"max_num_sentences\": config.max_num_sentences,\n                    \"max_length\": config.max_length,\n                    \"dropout_rate\": config.dropout_rate,\n                    \"gamma\": config.gamma,\n                    \"fssa_params\": {\n                        \"linear_rank\": config.fssa_linear_rank,\n                        \"emb_rank\": config.fssa_emb_rank,\n                        \"linear_sparsity\": config.fssa_linear_sparsity,\n                        \"emb_sparsity\": config.fssa_emb_sparsity,\n                        \"block_size\": config.fssa_block_size\n                    },\n                    \"qlora_params\": {\n                        \"r\": config.qlora_r,\n                        \"alpha\": config.qlora_alpha,\n                        \"dropout\": config.qlora_dropout\n                    }\n                }\n            }, f, indent=2)\n\n        with open(os.path.join(checkpoint_path, \"metrics.json\"), \"w\") as f:\n            json.dump(metrics, f, indent=2)\n\n        print(f\"Checkpoint saved to {checkpoint_path}\")\n        return checkpoint_path\n\n    except Exception as e:\n        print(f\"Error saving checkpoint: {str(e)}\")\n        raise\n\ndef upload_to_huggingface(save_path, repo_id):\n    \"\"\"Upload model to Hugging Face Hub\"\"\"\n    try:\n        create_repo(repo_id, exist_ok=True, token=True)\n        upload_folder(\n            repo_id=repo_id,\n            folder_path=save_path,\n            commit_message=\"Hybrid QLoRA + FSSA Hierarchical Legal Model\",\n            repo_type=\"model\",\n            token=True\n        )\n        print(f\"Model uploaded to https://huggingface.co/{repo_id}\")\n    except Exception as e:\n        print(f\"Upload failed: {str(e)}\")\n\ndef main():\n    \"\"\"End-to-end training pipeline\"\"\"\n    try:\n        start_time = time.time()\n        accelerator = Accelerator()\n        device = accelerator.device\n        print(f\"\\n{'='*50}\")\n        print(f\"STARTING HYBRID QLORA + FSSA HIERARCHICAL LEGAL MODEL TRAINING\")\n        print(f\"Timestamp: {datetime.now().isoformat()}\")\n        print(f\"Device: {device}\")\n        print(f\"Accelerator: {accelerator.state}\")\n        print(f\"{'='*50}\\n\")\n\n        os.makedirs(Config.output_dir, exist_ok=True)\n\n        train_hier, dev_hier, test_hier, label2id, id2label, label_list = prepare_hierarchical_datasets(\n            train_ds, dev_ds, test_ds\n        )\n\n        class_weights = compute_class_weights(train_hier, label2id).to(device)\n        print(f\"Class weights: {class_weights.cpu().numpy()}\")\n\n        train_tokenized, dev_tokenized, test_tokenized, tokenizer = tokenize_datasets(\n            train_hier, dev_hier, test_hier, label2id\n        )\n        train_loader, dev_loader, test_loader = create_data_loaders(\n            train_tokenized, dev_tokenized, test_tokenized\n        )\n\n        model = ImprovedHSLNModel(\n            num_labels=len(label2id),\n            class_weights=class_weights\n        ).to(device)\n\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=Config.learning_rate,\n            weight_decay=Config.weight_decay\n        )\n\n        # Enable gradient checkpointing for memory efficiency\n        model.bert.gradient_checkpointing_enable()\n        model.bert.enable_input_require_grads()\n\n        # Prepare with accelerator\n        model, optimizer, train_loader, dev_loader = accelerator.prepare(\n            model, optimizer, train_loader, dev_loader\n        )\n\n        model, history = train_model(\n            model, train_loader, dev_loader,\n            optimizer, device, Config.epochs, label_list\n        )\n\n        print(\"\\nEvaluating on training set...\")\n        train_metrics = evaluate_metrics(model, train_loader, device, label_list)\n\n        print(\"\\nEvaluating on dev set...\")\n        dev_metrics = evaluate_metrics(model, dev_loader, device, label_list)\n\n        metrics = {\n            \"train\": train_metrics,\n            \"dev\": dev_metrics,\n            \"overfitting_gap\": train_metrics[\"weighted_f1\"] - dev_metrics[\"weighted_f1\"],\n            \"model_size_mb\": get_model_size_mb(model),\n            \"training_memory_footprint_gb\": get_memory_footprint(),\n            \"label2id\": label2id,\n            \"id2label\": id2label,\n            \"training_time\": time.time() - start_time,\n            \"training_history\": history\n        }\n\n        checkpoint_path = save_checkpoint(\n            model, tokenizer, metrics, label2id, Config,\n            Config.output_dir, Config.save_checkpoint\n        )\n        upload_to_huggingface(checkpoint_path, Config.hf_repo_id)\n\n        print(\"\\n==== FINAL METRICS ====\")\n        print(f\"Train Weighted F1: {train_metrics['weighted_f1']:.4f}\")\n        print(f\"Dev Weighted F1:   {dev_metrics['weighted_f1']:.4f}\")\n        print(f\"Overfitting Gap:   {metrics['overfitting_gap']:.4f}\")\n        print(f\"Model Size:        {metrics['model_size_mb']:.2f} MB\")\n        print(f\"Training Time:     {metrics['training_time']:.2f} seconds\")\n        print(f\"Saved to:          {checkpoint_path}\")\n\n        print(\"\\nPer-class F1 Scores (Dev Set):\")\n        for label, score in dev_metrics[\"per_label_f1\"].items():\n            print(f\"{label}: {score:.4f}\")\n\n        print(f\"\\n{'='*50}\")\n        print(\"TRAINING PIPELINE COMPLETED SUCCESSFULLY\")\n        print(f\"{'='*50}\")\n\n        return metrics\n\n    except Exception as e:\n        print(f\"\\n{'!'*50}\")\n        print(\"PIPELINE FAILED!\")\n        print(f\"Error: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n        with open(os.path.join(Config.output_dir, \"error_log.txt\"), \"w\") as f:\n            f.write(f\"Pipeline error at {datetime.now()}\\n\")\n            f.write(str(e))\n            f.write(\"\\n\\nTraceback:\\n\")\n            f.write(traceback.format_exc())\n        return None\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}